### 📦 **Category 1: 코드베이스 심화 질문 (당신의 구현)**

#### **Q1: "N-player를 지원했는데, 왜 이런 결정을 했나요?"**

**💡 모범 답변:**
```
"과제 요구사항은 2인 게임이었지만, 3가지 이유로 N-player를 지원했습니다:

1. **Real-world thinking**: 실제 프로덕션에서는 요구사항이 변경됩니다. 
   2인 전용으로 만들었다가 나중에 3인 게임 요청이 오면 
   전체 리팩토링이 필요합니다.

2. **Cost vs Benefit**: 추가 비용이 크지 않았습니다. 
   `Player[]` 배열로 바꾸는 것만으로 확장성을 얻었고,
   테스트 복잡도도 크게 증가하지 않았습니다.

3. **Your role description**: JD에서 'integrations and tools'를 
   강조하셨는데, 통합 도구는 다양한 use case를 지원해야 합니다.
   저는 flexibility를 중요하게 생각합니다.
```

---

#### **Q2: "GameConfig 패턴을 사용했는데, 다른 방법은 고려했나요?"**

**💡 모범 답변:**
```
"네, 3가지 대안을 고려했습니다:

1. **YAML Configuration** (초기 고려)
   - 장점: 비개발자도 수정 가능
   - 단점: 런타임 에러 가능, compareFn 같은 함수 주입 불가
   - 결론: 이 과제에는 과함

2. **TypeScript Config Object** (최종 선택) ✅
   - 장점: 컴파일 타임 타입 체킹, 함수 주입 가능
   - 단점: 코드 재배포 필요
   - 결론: 테스트 가능성 + 타입 안전성 최적

SAP AI Hub처럼 여러 모델/설정을 지원하는 환경에서는
Config Pattern이 유용할 것 같습니다."
```

**🔥 Follow-up 준비:**
- "그럼 실제 프로덕션에서는?" → "Runtime config와 혼합 (default는 TS, override는 환경변수)"

---

#### **Q3: "테스트 커버리지가 92.85% (branches)인데, 나머지 7%는?"**

**💡 모범 답변:**
```
"나머지는 극단적인 엣지 케이스입니다:

1. **Game.ts line 32-47**: 모든 플레이어가 동시에 카드 소진
   - 실제 게임에서는 불가능 (한 명씩 소진)
   - 테스트 작성 시간 대비 가치가 낮다고 판단

2. **Error handling**: 생성자에서 throw하는 에러
   - 정상 플로우에서는 발생 안함
   - Integration test에서 일부 커버

100% 커버리지가 목표가 아니라, 의미 있는 테스트가 목표입니다.
저는 Task 1-3의 요구사항은 100% 커버했습니다.

만약 production이라면:
- Mutation testing (Stryker) 추가
- Edge case에 defensive programming 적용
- Error tracking (Sentry 등) 연동"
```

---

#### **Q4: "Fisher-Yates를 utils로 추출했는데, 이게 정말 필요했나요?"**

**💡 모범 답변:**
```
"처음에는 Deck.ts와 Player.ts에 각각 구현했습니다.
그런데 코드 리뷰 중 DRY 원칙 위반을 발견했습니다.

리팩토링 이유:
1. **중복 제거**: 동일한 알고리즘이 2곳에
2. **테스트 용이**: 한 곳만 테스트하면 됨
3. **버그 위험 감소**: 한 곳만 수정하면 됨

실제로 SAP AI Hub에서도 비슷한 상황이 많을 것 같습니다:
- API 호출 로직
- 에러 핸들링 패턴
- Retry mechanism

공통 로직은 shared utility로 추출하는 게 유지보수에 좋습니다."
```

---

### 🚀 **Category 2: SAP AI Hub 관련 질문**

#### **Q5: "우리 팀은 AI 통합 도구를 만듭니다. 어떤 도전이 있을까요?"**

**💡 모범 답변:**
```
"제가 생각하는 3가지 도전:

1. **Multiple AI Models 지원**
   - OpenAI GPT-4, Azure OpenAI, SAP Joule 등 다양한 모델
   - 각각 다른 API, 다른 응답 형식
   - 해결: Adapter Pattern (제 GameConfig처럼)

2. **Rate Limiting & Quota Management**
   - AI API는 비싸고 제한적
   - 해결: Queue system, caching, fallback

3. **Consistency across enterprise apps**
   - JD에서 'homogenous AI experience' 언급
   - 해결: Shared SDK/library, standardized interfaces

제 카드 게임에서 GameConfig로 여러 타입을 지원한 것처럼,
실제로는 ModelConfig로 여러 AI 모델을 추상화할 것 같습니다."
```

---

#### **Q6: "CI/CD 경험이 있나요? 이 프로젝트에 어떻게 추가할 건가요?"**

**💡 모범 답변:**
```
"네, 제 package.json에 'ci' 스크립트가 있습니다:

현재 구현:
npm run ci = lint + typecheck + test:coverage

GitHub Actions로 확장한다면:

```yaml
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      - run: npm run ci
      - uses: codecov/codecov-action@v3
```


#### **Q10: "이 포지션에서 가장 배우고 싶은 기술은?"**

**💡 모범 답변:**
```
"3가지를 배우고 싶습니다:

1. **Generative AI Integration** (JD에서 'nice to have')
   - 현재: OpenAI API 사용 경험만
   - 배우고 싶은 것: Prompt engineering, RAG, fine-tuning
   - 왜: SAP Joule 같은 엔터프라이즈 AI 통합의 best practice

2. **Cloud Native Patterns** (JD 핵심)
   - 현재: 모놀리식 앱 경험 많음
   - 배우고 싶은 것: Kubernetes, service mesh, observability
   - 왜: SAP AI Hub는 클라우드 네이티브일 것 같아서

3. **Enterprise-scale TypeScript** (JD 핵심)
   - 현재: 소규모 프로젝트 경험
   - 배우고 싶은 것: Monorepo, shared libraries, versioning
   - 왜: 'enable teams/partners/customers'는 대규모 codebase

이 포지션이 이 3가지를 모두 배울 수 있는 기회라고 생각합니다!"
```

**🎯 포인트:** JD 키워드를 자연스럽게 연결

---

## 🎓 **Category 5: 당신이 물어야 할 역질문**

### **기술적 질문 (당신의 성장)**
1. "현재 Gen AI Hub의 기술 스택은 무엇인가요?"
2. "팀의 코드 리뷰 프로세스는 어떻게 되나요?"
3. "온보딩 시 어떤 프로젝트부터 시작하나요?"

### **팀/문화 질문**
4. "Potsdam 팀의 크기와 구성은?"
5. "스타트업 문화라고 하셨는데, 구체적으로 어떤 점인가요?"
6. "다른 SAP AI Core 팀과 협업이 많나요?"

### **임팩트 질문 (당신의 동기 어필)**
7. "제가 6개월 안에 기여할 수 있는 가장 큰 임팩트는?"
8. "팀의 올해 가장 큰 도전은 무엇인가요?"