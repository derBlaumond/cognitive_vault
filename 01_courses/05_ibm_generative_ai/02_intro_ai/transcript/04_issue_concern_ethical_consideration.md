## Ethical Considerations and Responsible Use of AI
Welcome to the Ethical Considerations and Responsible Use of AI. After watching this video, you'll be able to explain key ethical principles in AI. You'll also be able to develop strategies for responsible AI implementation. Let's begin our journey by looking into this case study. In 2014, Amazon developed an AI tool to automate the shortlisting of job applicants, but the system exhibited gender bias by penalizing female-associated terms due to the predominance of male candidates in the training data. Despite efforts to correct the bias, Amazon discontinued the tool in 2015 to uphold its commitment to diversity and equality. This case study clearly indicates the needs for ethical considerations in AI development to ensure these powerful technologies benefit society while minimizing potential harm. 

AI has transformed industries by enhancing efficiency, innovation, and decision-making processes. From healthcare and education to finance and entertainment, AI systems can analyze vast amounts of data, identify patterns, and make predictions that can save lives, improve services, and drive economic growth. However, with great power comes great responsibility. So let's understand the key aspects of ethical AI and responsible use. One of the foremost ethical concerns is data privacy and security. AI systems rely heavily on data to function effectively. This data often includes sensitive personal information. 

Therefore, it is imperative to implement robust data protection measures to prevent unauthorized access and breaches. It is advisable for organizations to comply with data protection regulations like the General Data Protection Regulation or GDPR and the California Consumer Privacy Act or CCPA, ensuring that user content is obtained and data is anonymized where possible. According to a New York Times article in 2020, a facial recognition company, Clearview AI, was found to have collected billions of images from social media and other websites without users' consent. They provided facial recognition software to law enforcement agencies, causing concerns about privacy violations. This incident led to a big debate about the ethical use of AI and a need for stronger privacy protections. AI systems can accidentally make existing biases in their training data even worse. These biases can lead to unfair treatment and discrimination in various applications, such as hiring, lending, and law enforcement. 

To mitigate this, developers should use diverse datasets, implement fairness-aware algorithms, and continuously audit AI systems for biased outcomes. Ensuring fairness in AI is not just a technical challenge, but a societal imperative. Another important aspect is transparency and accountability. Transparency in AI operations is essential for building trust. Users should be informed about how AI systems make decisions, what data they use, and how their personal information is handled. Additionally, the clear lines of accountability are essential. If an AI system causes harm or makes an incorrect decision, it should be clear who is responsible and how the issue will be rectified. 

Establishing such accountability framework is crucial for ethical AI deployment. As AI systems become more autonomous, the need for human oversight remains paramount. Fully autonomous systems, especially in critical areas like healthcare, transportation, and the military, should be designed to include human-in-the-loop mechanisms. These mechanisms ensure that human judgment and ethical considerations are integral to decision-making processes, thereby preventing potentially dangerous or unethical outcomes. Access to AI technology and its benefits should be equitable. If access is limited to only a privileged few, AI could exacerbate existing inequalities. Efforts are required to democratize AI, ensuring that it is accessible and beneficial to all segments of society, including underserved and marginalized communities. 

This includes investing in education and infrastructure to bridge the digital divide. Different applications of AI pose unique ethical challenges. For example, in healthcare, it is essential that AI prioritizes patient welfare and confidentiality. In law enforcement, the use of AI for surveillance and predictive policing raises significant concerns about privacy and civil liberties. Each application area requires tailored ethical guidelines to address its specific challenges and ensure responsible use. Another critical consideration is AI's environmental impact. Training and running large AI models consumes substantial energy, contributing to carbon emissions. 

It is crucial for developers and organizations to create energy-efficient algorithms and adopt sustainable practices. This includes leveraging advancements in hardware, optimizing algorithms for energy efficiency, and utilizing renewable energy sources for data centers. Ethical AI is not a one-time achievement, but a continuous process. It is essential to monitor AI systems and their impacts regularly, and feedback mechanisms should be established to gather input from users and stakeholders. Continuous improvement practices should be in place to update AI systems based on new ethical insights, technological advancements, and societal needs. In this video, you learned about the ethical considerations of AI, like data privacy and security, bias and fairness, transparency and accountability, autonomous systems and human oversight, and access and equality. You also learned about how to responsibly use AI by using diverse datasets, designing fully autonomous systems that include human-in-the-loop mechanisms, and ensuring AI's accessibility and benefits to all segments of society, optimizing algorithms for energy efficiency, and utilizing renewable energy sources for data centers.

## Considerations Around Generative AI
[MUSIC] Welcome to considerations around generative AI. After watching this video, you will be able to identify the complexities of copyright and content ownership in generative AI. You will also be able to explain the importance of privacy and confidentiality of data while using AI models. Further, you will explore the accuracy of generative AI outputs and understand the potential for hallucinations. Lastly, you will identify ethical considerations when developing and using generative AI technologies. Generative AI is transforming industries, pushing the boundaries of creativity and innovation. However, as with any powerful tool, it comes with its own set of challenges and considerations. 

In 2018, a piece of artwork titled Edmond de Bellamy, created by an AI algorithm called GAN, Generative Adversarial Network, was sold at an auction for over $400,000. This raised questions about who owned the rights to the artwork, whether it was the developers who programmed the AI or the buyers of the artwork. The sales sparked debates and discussions around intellectual property rights in AI-generated creations. The question of intellectual property and ownership in AI is complex. Who owns the right to AI generated content or innovations? As AI systems become more creative, this question will become increasingly pertinent. It is recommended to establish clear guidelines and policies to address these issues, balancing the interests of AI developers, users, and the broader society. Developing appropriate regulations to govern the use of AI while promoting innovation is a delicate balance that needs careful consideration. 

Clarifying intellectual property rights related to AI generated content is a complex and evolving issue. It is important to ensure that AI systems comply with existing laws and regulations, such as those related to data protection and consumer rights. Another critical concern in generative AI is privacy and confidentiality. Generative AI models often require large amounts of data for training, and this data can sometimes include sensitive or personal information. Implementing strong data protection measures is essential to ensure that the information used to train AI models does not violate individual privacy. To address privacy concerns, organizations are increasingly turning to private AI. Private AI refers to an AI environment created specifically for a particular organization intended for exclusive use by that organization. 

These are designed to keep data confidential and secure, preventing unauthorized access and misuse. Generative AI is incredibly powerful, but it still comes with its flaws. Sometimes, AI can produce outputs that are inaccurate or completely fabricated, a phenomenon known as hallucination. For instance, an attorney used ChatGPT to craft a legal motion filled with phony judicial opinions and legal citations. The attorney, unaware of ChatGPT's ability to fabricate cases, faced sanctions for submitting the document. The phenomenon of hallucination highlights the importance of critical thinking when using AI-generated content. It is crucial to always validate and fact check the information generated by AI to avoid the spread of misinformation. 

It is essential for developers and users to implement strategies to minimize the risk of AI hallucinations. This might include improving the quality of training data, refining algorithms, and establishing robust validation processes. Organizations can use private GPT models to address hallucination concerns. For instance, CustomGPT.ai is an advanced AI platform that develops tailored GPT chatbots using your business content, delivering accurate and reliable GPT-4 responses based on that content without fabricating information. By leveraging private models, businesses can reduce hallucination in AI outputs by ensuring the training data is highly relevant and accurate, leading to more reliable and context specific responses. High quality, diverse datasets are necessary for training effective models and the quantity and quality of data also play a significant role in the model's performance. More complex models can also achieve better performance, but also require more computational resources and may be harder to interpret. 

The ability to scale the use of generative AI across different applications and environments is important for widespread adoption. AI models should be robust to various inputs and able to handle unexpected scenarios gracefully. Finally, let's consider the ethical implications of generative AI. From continuing biases to potential misuse, the ethical landscape of AI is complicated. AI systems can unintentionally continue biases from their training data, leading to unfair results. It is important for developers to prioritize fairness, accountability, and transparency. This involves actively working to find and fix biases, and ensuring that AI systems are used responsibly. 

Generative AI can be misused to create deepfakes, which can have serious consequences. Deepfakes are videos, images, or audio recordings that have been manipulated using generative AI models to make them appear as if they are real. They can be very convincing, and they can be used for a variety of bad purposes, such as creating fake news and propaganda, spreading disinformation, harassing or blackmailing people, or committing fraud. Deepfakes can be used to create forged identity documents or photos for fraudulent purposes, such as opening bank accounts, obtaining loans, or gaining access to secure locations. This can lead to financial fraud and other security breaches. Ethics in AI extends beyond preventing harm. It's also about fostering positive outcomes and ensuring that AI technologies benefit society as a whole. 

This includes using AI to address social challenges, improve access to resources, and enhance quality of life. It is important for developers and organizations to create AI systems that support initiatives promoting social justice, environmental sustainability, and human well being. In this video, you learned about the copyright and ownership challenges faced with content created using generative AI and how important it is for developers and users to familiarize themselves with legal frameworks and stay updated on evolving laws. You also learned how to maintain the privacy and confidentiality of user data using private AI models and a combination of technical and legal measures. Further, you learned why critical thinking is important in ensuring the accuracy of generative AI outputs, and how it can mitigate AI flaws like hallucinations. Finally, you gained insights into the ethical considerations urging developers to prioritize fairness, accountability, and social benefits in AI development.

## Why Large Language Models Hallucinate
I'm going to state three facts, your challenge is to tell me how they're related. They're all space and aviation themed, but that's not it. Here we go. Number 1, the distance from the earth to the moon is 54 million kilometers. Number 2, before I worked at IBM, I worked at a major Australian airline, and Number 3, the James Web telescope took the very first pictures of an exoplanet outside of our solar system. What's the common thread? Well, the answer is that all three facts are an example of an hallucination of a large language model, otherwise known as an LLM. 

Things like ChatGPT, and Bing Chat. Fifty four million K, that's the distance to Mars, not the moon. It's my brother that worked at the airline not me, and infamously, at the announcement of Google's LLM Bad, it hallucinated about the web telescope. The first picture of exoplanet was actually taken in 2004. Now, while large language models can generate fluent and coherent texts on various topics and domains, they are also prone to just make stuff up. Plausible sounding nonsense. Let's discuss first of all what a hallucination is. 

We'll discuss why they happen, and we'll take some steps to describe how you can minimize hallucinations with LLMs. Now, hallucinations are outputs of LLMs that deviate from facts or contextual logic, and they can range from minor inconsistencies to completely fabricated or contradictory statements, and we can categorize hallucinations across different levels of granularity. Now, at the lowest level of granularity, we could consider sentence contradiction. This is really the simplest type, and this is where an LLM generates a sentence that contradicts one of the previous sentences. The sky is blue today. The sky is green today. Another example would be, prompt contradiction. 

This is where the generated sentence contradicts with the prompt that was used to generate it. If I ask an LLM to write a positive review of a restaurant and it returns, the food was terrible and the service was rude, that would be in direct contradiction to what I asked. Now, we already gave some examples of another type here, which is factual contradictions. These factual contradictions of factual error hallucinations are really just that absolutely nailed on facts that it got wrong. Barack Obama was the first president of the United States, something like that. Then there are also nonsensical or other relevant kind of information based hallucinations, where it just puts in something that really has no place being there, like the capital of France Paris, Paris is also the name of a famous singer. Thanks. Now, with the question of what LLM hallucinations are answered, we really need to answer the question of why, and it's not an easy one to answer because the way that they derive their output is something of a black box, even to the engineers of the LLM itself, but there are a number of common causes. 

Let's take a look at a few of those. One of those is data quality. Now, LLMs are trained on a large corpora of text that may contain noise, errors, biases, or inconsistencies. For example, some LLMs were trained by scraping all of Wikipedia and all of Reddit. Is everything on Reddit 100% accurate? Well, look, even if it was, even if the training data was entirely reliable, that data may not cover all of the possible topics or domains that LLMs are expected to generate content about, so LLMs may generalize from data without being able to verify its accuracy or relevance, and sometimes it just gets it wrong. As LLM reasoning capabilities improve, hallucinations tend to decline. 

Now, another reason why hallucinations can happen is based upon the generation method. Now, LLMs use various methods and objectives to generate text, such as beam search, sampling, maximum likelihood estimation or reinforcement learning. These methods, and these objectives may introduce biases and trade offs between things like fluency and diversity between coherence and creativity or between accuracy and novelty. For example, beam search may favor high probability, but generic words over low probability, but specific words. Another common cause for hallucinations is input context. This is one we can do something directly about as users. Now, here context refers to the information that is given to the model as an input prompt. 

Context can help guide the model to reduce the relevant and accurate outputs, but it can also confuse or mislead the model if it's unclear or if it's inconsistent or if it's contradictory. For example, if I ask an LLM chat bot, can cats speak English? I would expect the answer no. Do you need to sit down for a moment? But perhaps I just forgotten to include a crucial little bit of information, a bit of context. That this conversation thread is talking about the Garfield cartoon strip, in which case the LLM should have answered, yes, cats can speak English, and that cat is probably going to ask for second helpings of Lasagna. Context is important. 

If we don't tell it, we're looking for generated texts suitable for an academic essay or a creative writing exercise, we can't expect it to respond within that context. Which brings us nicely to the third and final part, what can we do to reduce hallucinations in our own conversations with LLMs? One thing we can certainly do is provide clear and specific prompts to the system. Now, the more precise and the more detailed the input prompt, the more likely the LLM will generate relevant and most importantly here, accurate outputs. For example, instead of asking, what happened in World War II? That's not very clear, it's not very specific. We could say, can you summarize the major events of World War II, including the key countries involved and the primary causes of the conflict? 

Something like that that really gets at what we are trying to pull from this. That gives the model a better understanding of what information is expected in the response. We can employ something called active mitigation strategies, and what these are, are using some of the settings of the LLM, such as settings that control the parameters of how the LLM works during generation. A good example of that is the temperature parameter, which can control the randomness of the output. A lower temperature will produce more conservative and focused responses while a higher temperature will generate more diverse and creative ones. But the higher the temperature, the more opportunity for hallucination. Then one more is multi shot prompting. 

In contrast to single shot prompting, where we only give one prompt, multi shot prompting provides the LLM with multiple examples of the desired output format or context, and that essentially primes the model, giving a clearer understanding of the user's expectations. By presenting the LLM with several examples, we help it recognize the pattern or the context more effectively. This can be particularly useful in tasks that require a specific output format, so generating code, writing poetry or answering questions in a specific style. While large language models may sometimes hallucinate and take us on an unexpected journey, 54 million kilometers off target. Understanding the causes and employing the strategies to minimize those causes really allows us to harness the true potential of these models and reduce hallucinations.

## Perspective of Key Players Around AI Ethics
[MUSIC] Welcome to the perspective of key players around AI ethics. After watching this video, you'll be able to discuss the approaches and principles adopted by various organizations to ensure ethical AI adoption. You will also be able to explain IBM's pillars of trust, Microsoft's approach to ethical AI, and Google's AI principles. Ensuring AI ethics is a complex issue that organizations are addressing with different approaches. Organizations like IBM, Microsoft, and Google have created approaches and tools to handle ethical concerns in AI development and use. Let's look at how these organizations are making sure AI is ethical through their unique methods and initiatives. In its commitment to ethical AI, IBM has made significant progress in promoting AI ethics through its pillars of trust. 

IBM's pillars of trust are focus areas that help us take action to build and use AI ethically. These pillars are explainability, fairness, robustness, transparency, and privacy. AI is explainable when it can show how and why it arrived at a particular outcome or recommendation. AI is fair when it treats individuals or groups equitably. AI is robust when it can effectively handle exceptional conditions like abnormal input or adversarial attacks. AI is transparent when appropriate information is shared with humans about how the AI system was designed and developed. Because AI ingests so much data, it is crucial to be designed to prioritize and safeguard humans privacy and data rights. 

IBM has developed toolkits to support these pillars. AI explainability 360 helps understand and interpret AI models to ensure transparency. AI fairness 360 provides metrics and algorithms to detect and mitigate bias in AI systems. Adversarial robustness 360 helps enhance AI systems security against adversarial attacks. AI fact sheets 360 promotes transparency by documenting AI models, processes, and performance, and AI privacy 360 safeguards user data privacy within AI applications. These pillars form the foundation of IBM's approach to creating trustworthy AI that aligns with ethical principles and societal values. Let us now look at Microsoft's approach to ethical AI development and deployment. 

Microsoft emphasizes the involvement of designers, developers, data providers, and regulatory bodies in ensuring ethical AI. It has deployed human-in-the-loop systems that incorporate human oversight to enable intervention when needed. Microsoft practices continuous monitoring and improvement. It maintains ongoing evaluation of AI systems to address ethical concerns. Microsoft conducts regular audits to ensure compliance with ethical standards and regulations. It uses algorithmic impact assessments, or AIA, to identify and mitigate biases before deployment. It has established bodies like Microsoft's 8th committee, whose primary responsibility is to provide guidance on issues, technologies, processes, and best practices for responsible AI. 

Lastly, Microsoft has developed a responsible AI standard that outlines accountability measures including transparency, fairness, and human oversight. Moving on, let us now understand Google's principles of developing and deploying AI. These are, be socially beneficial, focusing on the fact that AI should benefit society and be aligned with widely accepted principles. Secondly, avoiding creating or reinforcing bias means ensuring fairness and avoiding unjust impacts. Then, ensuring AI is being built and tested for safety, which means maintaining safety and taking steps to mitigate risks. Further, being accountable to people. That emphasizes allowing appropriate human direction and control. 

Incorporating privacy design principles means protecting user privacy and data. Upholding high standards of scientific excellence, that is, maintaining rigor and integrity in scientific research. And ensuring that AI is being made available for use that accords with these principles, which means restricting use to applications consistent with these guidelines. Furthermore, Google's AI governance includes a review process to assess AI projects for ethical risks and compliance with its AI principles. This process involves a multidisciplinary team of experts evaluating projects at multiple stages. In addition to following stringent principles, organizations are engaging with external stakeholders to ensure the ethical use of AI. For instance, the AI alliance, co-launched by IBM and Meta, alongside leading organizations from various sectors, is dedicated to fostering an open community aimed at accelerating responsible AI innovation. 

This initiative prioritizes scientific rigor, trust, safety, security, diversity, and economic competitiveness. By encompassing a broad range of organizations, the AI alliance ensures that AI education, research, development, deployment, and governance are conducted responsibly and inclusively. Benefiting everyone involved in the AI ecosystem. Apart from this, several consulting firms like Deloitte work with clients and external experts to develop ethical AI strategies. They provide consulting services to help organizations implement ethical AI practices. Another important aspect of developing ethical AI training and education programs is practiced by various organizations. For example, PwC offers training programs for employees on AI ethics. 

They focus on educating their workforce about the ethical implications of AI and how to implement ethical practices in their projects. Also, Deloitte provides workshops and resources to clients on AI ethics. They help organizations understand and address ethical challenges in AI deployment. Lastly, many organizations like Adobe engage in industry-wide initiatives like the partnership on AI to collaborate on ethical AI standards. They contribute to developing guidelines and best practices for responsible AI use. In this video, you learned about IBM's pillars of trust. These pillars are central to IBM's approach ensuring that AI systems are explainable, fair, robust, transparent, and privacy preserving. 

You also learned about Microsoft's approach to ethical AI, which involves human in the loop systems, continuous monitoring and improvement, regular audits using AIA, internal review bodies, and responsible AI standards. You explored Google's principles focused on benefiting society, ensuring fairness, holding accountability, and upholding high standards of scientific excellence. [MUSIC]

## The Importance of AI Governance
If you have been following the news and artificial intelligence, you probably already know that this field is growing at an exponential pace. Each day we are hearing about new use cases in AI, use cases, and applications that we haven't even dreamed of in the past few years. But in the same news, you probably are also hearing about AI systems that have been deployed to production that are not yielding the expected outcomes. We are hearing about chatbots that have been misdirecting customers and employees into making the wrong decisions. We also heard about chatbots that are hallucinating responses to the customers, and we also heard about models that have been generating biased outcomes. There is no denying that there is huge potential in artificial intelligence, but premature deployment and adoption of AI systems could put the companies at a huge risk of reputational and financial loss. This exactly is the reason why artificial intelligence governance has become one of the most relevant and important topics today. 

Let's dive in and understand a bit more about AI governance. Let's start off by defining what it means. AI governance refers to a set of rules, standards, and processes that have been set in place in order to ensure the responsible and ethical development and deployment of artificial intelligence systems. Think of it as a set of guard rails to ensure the ethical use of these artificial intelligence systems so that we minimize the risk in the systems while maximizing the potential benefit. Now, we all know what the benefits are, of artificial intelligence. It leads to reduced costs, improved efficiency, and leads to automation of repetitive and manual tasks. While these benefits make artificial intelligence a highly important topic and most sought-after technology today, it is the risk in artificial intelligence that is making AI governance an even more important topic to be discussed. 

In order to understand what these risks are, let's in a very broad sense, understand what constitutes an AI system. Now, an AI system is a system that is designed to take in inputs and produce outputs that in some way mimic, augment, or aid human decision-making. At the heart of the system is what we call the AI model. Now, the goal of this model is to look at the input and generate an output that a human would normally do. How does an AI model do that? In order for this model to do exactly that, we need to supply it with data. Because we want it to mimic or augment or aid human decision-making, we need to supply it data that is human-generated. 

This data could be in any format. It could be structured data with columns and values, or it could be semi-structured data, such as XML files, or unstructured data, such as PDF documents, text documents, audio files, video files, etc. Now, this AI model, which you can think of as highly engineered code that uses complex mathematical algorithms, looks at the data, derives patterns from data, and learns how to mimic the human behavior that is expected from it. Now, since we are talking about data, and this data is human-generated. We humans, unfortunately, are not devoid of bias. We have several cognitive biases within us. An example of one such bias is, some of us tend to put undue importance on certain factors while making decisions while completely ignoring another set of factors. 

Now, that can cause biases in the data. Yes, the biases are not patently visible. You cannot look at the data and say, yes, I see biases, but these are latent and hidden. This highly engineered mathematical code has a tendency to pick up on these biases. In worst-case scenarios, it reflects those biases in the outcomes. That makes the AI systems susceptible to bias. Secondly, we are talking about data here. 

The data is being sent as input, data is being used for training the model, and if there is no proper oversight, this data could contain private and sensitive information. When there is no proper oversight, this data can seep into the model and seep into the output of the model, leading to privacy infringement. Or in other cases, in cases of unstructured data, it could also contain copyright information, and that can also be reflected in your model's output, so privacy or copyright infringement. Some of the models that we use are black box models. The reason we use these black box models as opposed to the glass box models is that black box models tend to provide a higher level of accuracy in the outcomes. When we use black box models, what it means is that the people who are creating these models and systems have little control over the inner workings of the algorithms. When you ask them why their model is making a certain decision, they won't be able to give you an explanation. 

In that case, your system is not transparent. That puts you at a risk of lack of transparency or trust. How would you trust a system that is not able to explain why and how it made a certain decision? That is the third risk in artificial intelligence systems. Now, these AI models are not something that you once create, and they continue to generate high-quality outcomes. These models can deteriorate, and the deterioration can happen because the incoming data could be very different from the data that the models have been trained on. Because this deterioration happens, there is a need for continuously monitoring these models. 

That is another factor that can put your models at risk because if you're not continuously monitoring them, then your model is not producing consistently high-quality outcomes. Because of risks like these, there are organizations across the world that are coming up with regulations and guidelines on how to manage these systems. Now, the guidelines, you can think of something like the NIST AI regulation, AI Risk Management Framework. As for regulations, we all heard about the EU AI Act. These regulations are much more serious because these are not just guidance for AI system deployment, but can actually penalize companies for non-compliance. When your company or when your AI systems are not complying with the guidelines stipulated by the regulation, then you will be at a risk of reputational loss and financial loss as well, and several ethical dilemmas as well. Due to factors such as these, bias, privacy, or copyright infringement, lack of trust or transparency, and lack of continuous monitoring, and the presence of regulations, it is extremely important that we govern our artificial intelligence systems. 

I think we can all agree that the promise of AI is undeniable, but the risks it poses are very much real, and a properly governed AI system is extremely important for organizations to fully realize the potential of artificial intelligence. I hope you found this video helpful.

## How to Implement AI Ethics
AI ethics are something that's on everyone's mind these days. How do enterprises determine if their AI solution is at risk of crossing some sort of ethical boundary? The first thing we want to do is come up with a set of guidelines or rules that we're going to follow whenever we are creating or interacting with AI systems. 1. Artificial intelligence is meant to augment human intelligence. It's not here to replace us. 2. Data and insights belong to their creator. 

If we are using customers' data for anything, that is, it's still their data, not ours. 3. Solutions have to be transparent and explainable, and what this means is that we need visibility into who's training the system, what data they're using to train the system, and then also how all of this is going to affect an algorithm's recommendations to the end user. Now we have our rules. How do we determine if we are actually following them or breaking any guidelines? There are a few different design thinking activities that you could try. One of them is called dichotomy mapping. 

Basically, what this means is that, first, we're going to list all of the features of our solution, and then we're going to list the benefits and what they're meant to be used for. I'll give you an example. Let's say a hotel has a recommendation system for their users that will determine maybe what room they get, or if we leave any treats for them in their room, if someone's going sky diving, maybe we give them a room on a higher floor. We list all of these benefits out, and obviously, these things are great for the customer. They're getting a great experience. But then, the next thing we need to do is actually look at these features and determine, can this cause harm in any way? In this case we would want to look at, is this data being sold to advertisers? Is it secure? 

Something else, if we have differently abled users, are they able to use the interface? Are they being included in the algorithm? Once we've got our rules, we've got our activities, we've defined some issues that maybe we need to work on, what do we do next? We fix it. The first thing we're going to do is implement a set of guard rails, and these are basically just rules that your AI system has to follow. In this case, it's going to be we do not sell to advertisers. That is a guard rail. 

Next, let's talk about the data that we're using to train this AI system. As I mentioned before, if we're not using a diverse set of data, then we're actually not going to be able to accommodate all of our users. The next thing we could do is look at some open-source schooling. IBM actually has one called AI Fairness 360, and it's actually going to help you mitigate and detect bias in your machine learning models. There may be other tools that help you with adhering to privacy regulations or even detecting uncertainty in your models. Now that we have our rules, we know how to identify problems and then how to fix them. AI is all of our responsibility. 

We have to make sure that the AI that we're creating and using is safe, secure, and built by humans with humans in mind.