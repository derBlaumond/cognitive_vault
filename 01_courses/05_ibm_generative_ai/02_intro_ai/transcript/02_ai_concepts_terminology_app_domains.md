## Cognitive Computing
Welcome to Cognitive Computing. After watching this video, you'll be able to explain the concept of cognitive computing and its core elements. You will also be able to analyze the impact of cognitive computing technologies on various industries. Remember those school textbook exercises that followed predetermined learning paths? They may lack the capability to adapt to individual learning styles. Cognitive computing is one such technology that can evaluate an individual's performance and offer personalized recommendations. It provides enhanced functionality, adaptability, and intelligence across various domains. 

The term cognitive involves intellectual activities, such as thinking, reasoning, and problem-solving. Similarly, cognitive computing aims to create systems that mimic these human cognitive processes. It is a branch within AI that mimics human thought processes to create intelligent machines. Cognitive computing makes machines active partners, not just tools. These advanced systems do more than execute commands. They grasp your needs, anticipate your questions, and proactively deliver valuable insights. For instance, the cognitive computing system can help a bank detect fraudulent transactions, or it can be employed by a company to improve its customer support through the chat bot. 

When we as humans seek to understand something and to make a decision, we go through four key steps. First, we observe visible phenomena and bodies of evidence. Second, we draw on what we know to interpret what we are seeing to generate a hypothesis about what it means. Third, we evaluate which hypotheses are right or wrong. Finally, we decide choosing the option that seems best and acting accordingly. Just as humans become experts by going through the process of observation, evaluation, and decision-making, cognitive systems use similar processes to reason about the information they read, and they can do this at massive speed and scale. Cognitive computing has three core elements: perception, learning, and reasoning. 

Let's see how these processes function to simulate decision-making and human-like intelligence. Perception is the basis of cognitive computing to interpret and understand the environment. The components of perception include sensing, where data, whether structured or unstructured, is gathered from various sources. Cognitive computing employs machine learning algorithms to analyze the data and extract meaningful information from the data. By analyzing patterns and trends within the data, the system gains a detailed idea of complex relationships and makes accurate predictions. This repetitive process highlights the evolving nature of cognitive computing. Some of the key benefits of cognitive computing include elevated decision-making due to the analysis of huge datasets. 

It improves efficiency, as it saves time and resources by automating tasks. Additionally, it enables more human-like and interactive communication between machines and humans with the help of natural language processing. Cognitive computing has multiple applications from healthcare and finance to education and entertainment. Several companies across various industries are using cognitive computing technologies to enhance their products, services, and operations. Some notable examples include IBM Watson, which uses cognitive computing in the fields of healthcare, finance, retail, and customer service. Google utilizes cognitive computing techniques in its products and services, including Google Search, Google Assistant, and Google Translate. Amazon's Alexa virtual assistant employs cognitive computing to understand and respond to voice commands, manage smart home devices, and provide personalized recommendations to users. 

JPMorgan Chase and Wells Fargo use cognitive computing to detect fraud, assess risk, and automate customer service. In this video, you learned that cognitive computing technology mimics human cognitive processes like thinking, reasoning, and problem-solving. The core elements of cognitive computing include perception, which refers to gathering and interpreting data from various sources, learning, which refers to using machine learning algorithms to analyze data, and reasoning, which refers to making accurate predictions and decisions based on data analysis. The benefits of cognitive computing include improved decision-making, increased efficiency, and more interactive human machine communication. Companies like IBM, Google, Amazon, JPMorgan Chase, and Wells Fargo use cognitive computing to enhance their services and operations.

## Terminologies and Related Concepts of AI
Welcome to the terminologies and related concepts of AI. After watching this video, you will be able to explore the key terms associated with artificial intelligence. You will also be able to explain the various key concepts of artificial intelligence. AI enables a world where machines can not only understand human language, but also predict our needs, recognize our faces, and provide safety against threats. In this age of AI, understanding the language, key terms, and related concepts of AI becomes crucial. These will not only enhance your knowledge, but also enable you to leverage AI's full potential, drive innovation, and stay ahead in your field. For instance, autonomous vehicles or self driving cars rely heavily on AI technologies, such as machine learning, deep learning, natural language processing, and computer vision to navigate and make real-time decisions. Understanding these key terms can provide valuable insights into how these vehicles operate, their benefits, and their challenges. Before discussing various terminologies and key concepts of AI, let's begin with understanding AI. Artificial intelligence is a branch of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. AI systems will typically demonstrate behaviors associated with human intelligence, such as planning, learning, reasoning, problem solving, knowledge representation, perception, motion, manipulation, social intelligence, and creativity. 

AI is categorized into narrow AI, general AI, and super AI. Narrow AI or weak AI perform specific tasks. General AI, also known as strong AI, possesses human like cognitive skills and is capable of learning and adapting across various tasks, whereas super AI aims to surpass human intelligence and is currently theoretical and not yet realized. Now that you have understood what AI is, let's dive into the concept of machine learning. Machine learning is a subset of AI that uses computer algorithms to analyze data and make intelligent decisions based on what it has learned without being explicitly programmed. Machine learning algorithms are trained with large datasets and learn from examples. They do not follow rule based algorithms. 

Machine learning enables machines to solve problems on their own and make accurate predictions using the provided data. Moving on, let's discuss deep learning. Deep learning is an important concept and a specialized subset of machine learning. It uses multi-layered neural networks, known as deep neural networks to analyze complex data and simulate human decision making. Deep learning algorithms can label and categorize information and identify complex patterns in data. It enables AI systems to learn continuously and improve the quality and accuracy of results by evaluating the correctness of decisions. Another important concept is neural networks, a computation model inspired by the human brain's neural structure. Neural networks consist of interconnected nodes or neurons and have three layers. While the input layer receives and processes raw data, hidden layers perform complex computations and transform data. The output layer converts process data into output format, producing the result. In this video, you learned how AI simulates human intelligence and machines, and how it can be divided into three categories, weak AI for specific tasks, strong AI with human like cognitive abilities, and super AI, which aims to surpass human intelligence. You also learned how machine learning, a subset of AI, uses algorithms to analyze data, make decisions without explicit programming, and enables autonomous problem solving. Additionally, you learned that deep learning uses neural networks with multiple layers to analyze complex data. Finally, you learned about neural networks, a computational model consisting of interconnected nodes with three layers.

## Machine Learning
Welcome to machine learning. After watching this video, you will be able to describe the fundamentals of machine learning and recognize its various types. Machine learning, a subset of AI, uses computer algorithms to analyze data and make intelligent decisions based on what it has learned. Instead of following rules-based algorithms, machine learning builds models to classify and make predictions from data. Let's understand this by exploring a problem we may be able to tackle with machine learning. What if we want to determine whether a heart can fail? Is this something we can solve with machine learning? 

The answer is yes. Let's say we are given datasets as beats per minute, body mass index, age, sex, and the result of whether the heart has failed or not. With machine learning, we can learn and create a model that, given inputs, will predict results. So what is the difference between this and using statistical analysis to create an algorithm? An algorithm is a mathematical technique. With traditional programming, we take data and rules and use these to develop an algorithm that will give us an answer. In the previous example, if we were using a traditional algorithm, we would take the data such as beats per minute and BMI and use this data to create an algorithm that will determine whether the heart will fail or not. 

Essentially, it would be an if-then-else statement. When we submit inputs, we get answers based on what the algorithm we determined is, and this algorithm will not change. Machine learning, on the other hand, takes data and answers and creates the algorithm. Instead of getting answers in the end, we already have the answers. What we get is a set of rules that determine what the machine learning model will be. The model determines the rules and the if-then-else statement when it gets the inputs. Essentially, what the model does is determine what the parameters are in a traditional algorithm, and instead of deciding arbitrarily that beats per minute plus BMI equals a certain result, we use the model to determine what the logic will be. 

This model, unlike a traditional algorithm, can be continuously trained and used in the future to predict values. Machine learning relies on defining behavioral rules by examining and comparing large datasets to find common patterns. For instance, we can provide a machine learning program with a large volume of pictures of birds and train the model to return the label bird whenever it has provided a picture of a bird. We can also create a label for cat and provide pictures of cats to train on. When the machine model is shown a picture of a cat or a bird, it will label the picture with some level of confidence. This type of machine learning is called supervised learning, where an algorithm is trained on human-labeled data. The more samples you provide a supervised learning algorithm, the more precise it becomes in classifying new data. 

Unsupervised learning, another type of machine learning, relies on giving the algorithm unlabeled data and letting it find patterns by itself. You provide the input, but not labels, and let the machine infer qualities that the algorithm ingests unlabeled data, draws inferences, and finds patterns. This type of learning can be useful for clustering data, where data is grouped according to how similar it is to its neighbors and dissimilar to everything else. Once the data is clustered, different techniques can be used to explore that data and look for patterns. For instance, you provide a machine learning algorithm with a constant stream of network traffic and let it independently learn the baseline, normal network activity, as well as the outlier and possibly malicious behavior happening on the network. The third type of machine learning algorithm, reinforcement learning, relies on providing a machine learning algorithm with a set of rules and constraints and letting it learn how to achieve its goals. You define the state, the desired goal, allowed actions, and constraints. 

The algorithm figures out how to achieve the goal by trying different combinations of allowed actions and is rewarded or punished depending on whether the decision was a good one. The algorithm tries its best to maximize its rewards within the constraints provided. You could use reinforcement learning to teach a machine to play chess or navigate an obstacle course. A machine learning model is an algorithm used to find patterns in the data without the programmer having to explicitly program these patterns. In this video, you learned that machine learning is a subset of AI that uses computer algorithms to analyze data and make decisions. Unlike traditional programming, which uses fixed rules, machine learning models evolve by learning from data. There are three main types of machine learning. 

Supervised learning, which is trained on labels to classify new data. The more samples you provide a supervised learning algorithm, the more precise it becomes in classifying new data. Unsupervised learning that finds patterns in unlabeled data. It is useful for clustering similar data points and detecting anomalies. And reinforcement learning that learns to achieve goals within a set of rules and constraints by maximizing rewards. It is applicable in tasks like playing chess or navigating obstacle courses.

## Machine Learning: Techniques and Training
Welcome to the Machine Learning: Techniques and Training. After watching this video, you will be able to explain the core concepts of machine learning, including supervised, unsupervised, and reinforcement learning techniques and discuss various supervised learning tasks such as regression, classification, and neural networks. You will also be able to describe the process of training a machine learning model, including the roles of training, validation, and test datasets. Machine learning is a broad field that can be divided into three categories: supervised learning, unsupervised learning, and reinforcement learning. There are many different tasks you can solve using these categories. Supervised learning involves using datasets with predefined class labels to train models for predicting or classifying new data points. This means the data you receive includes labels indicating what each data point represents. 

In this example, you have a table with labels such as age or sex. With unsupervised learning, you do not have class labels and you must discover class labels from unstructured data. This could involve deep learning, which looks at pictures in unstructured data, interprets them, and processes them, further grouping them into clusters based on their features. Reinforcement learning is a different subset, and what this does is it uses a reward function to penalize bad actions or reward good actions. Supervised learning can be split into three categories: regression, classification, and neural network. Regression models are built by looking at the relationships between features x and the result y, where y is a continuous variable. Essentially, regression estimates continuous values. 

Neural networks refer to structures that imitate the structure of the human brain to process input data, recognize patterns, and make decisions or predictions. Classification, on the other hand, focuses on discrete values it identifies. You can assign discrete class labels y based on many input features x. In the previous example, given a set of features x, such as beats per minute, body mass index, age, and sex, the algorithm classifies the output y as two categories, true or false, predicting whether the heart will fail or not. In other classification models, you can categorize results into multiple groups. For instance, you can predict whether a movie belongs to the genres of action, comedy, drama, or horror. Some forms of classification include decision trees, support vector machines, logistic regression, and random forests. 

Classification allows us to extract features from the data. The features in this example would be beats per minute or age. Features are distinctive properties of input patterns that help determine the output categories or classes of output. Each column is a feature, and each row is a data point. Classification is the process of predicting the class of given data points. Our classifier uses training data to understand how given input variables relate to that class. What exactly do we mean by training? 

Training refers to using a learning algorithm to determine and develop the parameters of your model. Imagine you are training a model to predict whether an email is spam or not, true for spam, false for not spam. To do this, you start by showing the algorithm some real-life emails that are labeled as true, indicating they are spam. Then you show more emails labeled as false, indicating they are not spam. You continue this process with emails labeled as true or false. The algorithm adjusts its internal parameters until it has learned to distinguish from the data which e-mails are spam, that is true, and which are not, that is false. In machine learning, a dataset is usually divided into three parts: training, validation, and test sets. 

The training set is used to train the algorithm. The validation set helps to validate the results and adjust the algorithm's parameters. The test set contains data the model has not seen before and is used to assess the model's performance. You can then measure the model's effectiveness using terms like accuracy, precision, and recall. In this video, you explored the three main categories of machine learning: supervised learning, which builds models using labeled data; unsupervised learning, which discovers patterns in unlabeled data, often through clustering; and reinforcement learning, which employs a reward system to guide actions. Additionally, you learned how supervised learning is divided into regression, classification, and neural networks. Regression estimates continuous values by analyzing the input output relationship. 

Neural networks imitate the human brain structure to process data, and classification focuses on discrete values by assigning class labels based on input features. Lastly, you learned how model training involves splitting the dataset into training, validation, and testing sets. The training set trains the algorithm. The validation set fine-tunes and validates it, and the testing set evaluates its performance on new data.

## Deep Learning
Welcome to Deep Learning. After watching this video, you will be able to describe the fundamentals of deep learning and recognize its various applications. While machine learning is a subset of artificial intelligence, deep learning is a specialized subset of machine learning. Deep learning layers algorithms to create a neural network, which is an artificial replication of the brain structure and functionality. This enables AI systems to continuously learn on the job and improve the quality and accuracy of results. It also allows these systems to learn from unstructured data, such as photos, videos, and audio files. Deep learning, for example, enables natural language understanding capabilities of AI systems and allows them to work out the context and intent of what is being conveyed. 

Deep learning algorithms do not directly map input to output, instead, they rely on several layers of processing units. Each layer passes its output to the next layer, which processes it and passes it to the next. The many layers are why it's called deep learning. When creating deep learning algorithms, developers and engineers configure the number of layers and the type of functions that connect the outputs of each other to the inputs of the next. Then they train the model by providing it with lots of annotated examples. For instance, you give a deep learning algorithm thousands of images and labels that correspond to the context of each image. The algorithm will run those examples through its layered neural network and adjust the weights of the variables in each layer of the neural network to be able to detect the common patterns that define the images with similar labels. 

Deep learning fixes one of the major problems present in older generations of learning algorithms. While the efficiency and performance of machine learning algorithms plateau as the datasets grow, deep learning algorithms continue to improve as they are fed more data. Deep learning has proven to be very efficient at various tasks, including image captioning, voice recognition and transcription, facial recognition, medical imaging, and language translation. Deep learning is also one of the main components of driverless cars. In this video, you learn that deep learning is a specialized subset of machine learning that layers algorithms to create a neural network. Deep learning enables AI systems to continuously learn and improve. Deep learning enhances AI's natural language understanding by grasping context and intent. 

Deep learning is highly effective in tasks like image captioning, voice recognition, facial recognition, medical imaging, language translation, and even driverless car technology.

## Neural Networks
[MUSIC] Welcome to Neural Networks. After watching this video, you will be able to describe neural networks and their functionality. You will also be able to recognize various types of neural networks. Imagine teaching a machine to learn in a manner like a human brain, like recognizing faces and images or driving cars independently. The mechanism behind such remarkable capabilities lies within neural networks. Neural networks are computational models influenced by the human brain's neural structure. An artificial neural network consists of interconnected nodes known as neurons. 

The neurons take incoming data, like the human brain's neurological network, and learn to make decisions over time. By providing the network with data such as images of cats and dogs, it can learn to recognize patterns and form connections. The more data it is exposed to, the more effectively it learns. A neural network consists of one input layer and one output layer. It can also have one or more hidden layers. The input layer receives the data. For example, in an image recognition task, the input layer would take in the image's pixel values. 

Hidden layers process the data. Each hidden layer transforms the input data by applying an activation function. Activation functions are mathematical functions that allow the network to learn complex patterns. Finally, the output layer of a neural network produces the final result of the network's processing. Note that as a network includes more hidden layers, it becomes deeper, which is why the term deep learning is used. Neural networks learn through a process called training. Let's understand the steps of this process. 

Firstly, as part of the forward pass or forward propagation step, the data passes through the layers of the neural network. In this step, the network computes an output. This output is compared to the actual answer to calculate the difference, called the error or loss. This step shows how well the network's predictions match the actual results. Then, in the backward pass or back propagation step, this error is sent back through the network to adjust the internal parameters such as weights and biases. This adjustment aims to reduce the error for future predictions. The forward pass, error calculation, and backward pass are repeated many times with different sets of data until the neural network consistently makes accurate predictions. 

There are various types of neural networks. Let's look at some, including perceptron neural networks, feed-forward neural networks, deep feed-forward neural networks, modular neural networks, convolutional neural networks, and recurrent neural networks. The perceptron neural networks are the simplest type of artificial neural network, consisting of only input and output layers. In a feed-forward neural network, information flows in one direction, that is, forward. Each neuron in a layer receives input from neurons in the previous layer and then passes its output to neurons in the next layer. A deep feed-forward neural network is similar to feed-forward network with just more than one hidden layer. The modular neural network combines two or more neural networks to arrive at the output. 

A convolutional neural network, or CNN, is a type of neural network that is particularly well suited for analyzing visual data. The term convolutional refers to a mathematical operation where a function is applied to another function, and the result is a mixture of the two functions. In CNNs, this process takes place through multiple layers, with each layer performing a convolutional on the output from the previous layer. In recurrent neural networks, or RNNs, each of the neurons in hidden layers receives an input with a specific delay in time. This allows the RNN to consider the context of the input. You can use this type of neural network where you need to access previous information in current iterations. For example, it's useful in predicting the next word in a sentence, as it considers the context and flow of a conversation. 

In this video, you learned that neural networks are building blocks of AI systems inspired by the structure of the human brain. Neural networks consist of layers, including an input layer, one or more hidden layers, and an output layer. Neural networks rely on forward propagation to process data from input to output layers. Backpropagation adjusts internal parameters based on the loss calculated during forward propagation. Various types of neural networks include perceptron neural network, feed-forward, deep feed-forward, modular, convolutional neural network, and recurrent neural network. [MUSIC]

## Machine Learning Vs. Deep Learning
[MUSIC] Look, fair warning, if you're feeling a little hungry right now, you might want to pause this video and grab a snack before continuing. Because I'm going to explain the difference between machine learning and deep learning by talking about pizza, delicious, tasty pizza. Now, before we get to that, let's address the fundamental question here. What is the difference between these two terms? Well, put simply, deep learning is a subset of machine learning. Actually, the hierarchy goes like this. At the top, we have AI, or artificial intelligence. 

Now a subfield of AI is ML, or machine learning. Beneath that, then we have NN, or neural networks, and they make up the backbone of deep learning algorithms, DL. Now, machine learning algorithms leverage structured labeled data to make predictions. So let's build one, a model to determine whether we should order pizza for dinner. There are three main factors that influence that decision, so let's map those out as inputs. The first of those inputs, we'll call X1, and X1 asks, will it save time by ordering out? We can say yes with a 1 or no with a 0. 

Yes, it will. So x that equals one. Now X2, that input says, will I lose weight by ordering pizza? That's a 0, I'm ordering all the toppings. And X3, will it save me money? Actually, I have a coupon for a free pizza today, so that's a one. Now, look, these binary responses, 1s and 0s, I'm using them for simplicity. 

But neurons in a network can represent values from, well, everything to everything, negative infinity to positive infinity. With our inputs defined, we can assign weights to determine importance. Larger weights make a single inputs contribution to the output more significant compared to other inputs. Now, my threshold here is 5, so let's weight each one of these. W1, well, I'm going to give this a full 5 because I value my time. W2, this was the will I lose weight one. I'm going to rate this a 3 because I have some interest in keeping in shape. 

And for W3, I'm going to give this a two, because either way, this isn't going to break the bank to order dinner. Now, we plug these weights into our model, and using an activation function, we can calculate the output, which in this case is the decision to order pizza or not. So, to calculate that, we're going to calculate the y hat and we're going to use these weights and these inputs. So here we've got one times five, we've got zero times three, and we've got one times two, and we need to consider as well our threshold which was five. So that gives us, if we just add these up, one times five, that's five plus zero times three, that's zero plus one times two, that's two minus five. Well, that gives us a total of +2. And because the output is a positive number, this correlates to pizza night. 

Okay, so that's machine learning, but what differentiates deep learning? Well, the answer to that is more than three, as in a neural network is considered a deep neural network if it consists more than three layers, and that includes the input and the output layer. So we've got our input and output, we have multiple layers in the middle, and this would be considered a deep learning network. Classical machine learning is more dependent on human intervention to learn. Human experts, well, they determine a hierarchy of features to understand the differences between data inputs. So, if I showed you a series of images of different types of fast food, like pizza, burger, and taco, you could label these in a dataset for processing by the neural network. A human expert here has determined the characteristics which distinguish each picture as the specific fast food type. 

So, for example, it might be the bread of each food type, might be a distinguishing feature across each picture. Now, this is known as supervised learning because the process incorporates human intervention or human supervision. Deep machine learning doesn't necessarily require a labeled dataset. It can ingest unstructured data in its raw form, like text and images, and it can automatically determine the set of features which distinguish pizza, burger, and taco from one another. By observing patterns in the data, a deep learning model can cluster inputs appropriately. These algorithms discover hidden patterns of data groupings without the need for human intervention, and they're known as unsupervised learning. Most deep neural networks are feed forward. 

That means that they go in one direction from the input to the output. However, you can also train your model through something called backpropagation. That is, it moves in the opposite direction from output to input. Backpropagation allows us to calculate and attribute the error associated with each neuron and allows us to adjust and fit the algorithm appropriately. So when we talk about machine learning and deep learning, we're essentially talking about the same field of study, neural networks. They're the foundation of both types of learning, and both are considered subfields of AI. The main distinction between the two are that number of layers in a neural network, more than three, and whether or not human intervention is required to label data.

## Generative AI Models
Welcome to Generative AI Models. After watching this video, you will be able to explain the concept of generative AI models and list their various types. Who thought machines could be so creative? Generative AI Models are making it possible to mimic human creativity for generating text, art, music, and video. Generative AI Models are a class of AI systems that learn from large data-sets by recognizing patterns and trends. By learning from the data, they use machine learning and deep learning algorithms to create new content. The design of a generative AI model changes depending on what it's designed to do and how it will be used. 

Some common types of generative AI models are variational autoencoders, or VAEs, generative adversarial networks, or GANs, autoregressive models, and transformers. VAEs work by transforming input data through encoding and decoding. They have three main parts: an encoder network, a latent space, and a decoder network. The encoder takes the input data and turns it into a simpler form called the latent space representation. It holds the key features of the data. The decoder then uses this latent space representation to create new outputs. The applications of a VAE model include image generation, anomaly detection, and so on. 

For example, the Fashion MNIST VAE model is used to generate and reconstruct images from the Fashion MNIST dataset, which includes different clothing items like shirts, shoes, and bags. It creates new, realistic images of clothing items based on the patterns it has learned from the data. GANs involve two neural networks : the generator and the discriminator. The generator creates new data samples, and the discriminator checks if the data is real or fake. Both networks train together. The generator tries to make data that looks real, and the discriminator tries to tell the difference between real and fake data. This process continues until the generator becomes so good at producing realistic data that the discriminator can no longer tell the difference. 

GANs can be used for image synthesis, style transfer, data augmentation, and so on. For example, GANs can be used in the fashion industry. A famous example is Nvidia's StyleGAN that can generate high-quality and realistic images of faces, animals, landscapes, and more. Autoregressive models create data sequentially, considering the context of earlier generated elements. These models predict the next element in the sequence based on the previous one. These models can generate sequences of data like text or music. For example, a music composition tool powered by an autoregressive model can generate new melodies. 

WaveNet is one of the examples of an autoregressive model that generates raw audio waveforms, producing high-quality, natural-sounding speech. Transformers are generally used in natural language processing or NLP tasks. They consist of encoder and decoder layers, enabling the model to effectively generate text sequences or perform cross-language translations. For example, a multilingual chatbot uses transformers to understand and respond to queries in multiple languages. Large language models like OpenAI's Generative Pre-trained Transformer or GPT family of models and Google Gemini models are examples of transformers that can generate different creative texts. Generative AI models can usually be categorized into unimodal models and multimodal models. Unimodal models process inputs and generate outputs within the same modality, whereas multimodal models handle inputs from one modality and produce outputs in a different modality. 

Modality refers to the type of data being processed. Unimodal models handle one type of data, while multimodal models process multiple types of data. For instance, GPT-3 is a unimodal model that takes text input and generates text output. It can complete sentences, generate stories, or answer questions based on textual prompts. Whereas DALL-E is a multimodal model that generates images from textual descriptions. For example, for a prompt like an elephant playing with a ball, DALL-E can create a corresponding image. Meta's ImageBind, another open-source multimodal AI model, processes diverse data types like text, audio, visual, and movement. 

Its unique ability to combine information across these modalities allows it to create art from mixed inputs. For instance, it can merge the sound of a flowing river with a cityscape visual, showcasing its versatility. As we move forward, these models will keep pushing the boundaries of creativity, revolutionizing industries, and enhancing human experiences. In this video, you will learn that generative AI models are a class of artificial intelligence systems that use algorithms to create new content. Generative AI model architectures include VAEs, GANs, autoregressive models, and transformers. VAEs use the encoder to convert the input data into latent space representation, then the latent space captures the characteristics of data, and finally, the decoder generates outputs. GANs use the generator to generate new data samples and the discriminator to verify the generated data. 

Autoregressive models create data sequentially, taking into account the context of earlier generated elements. Transformers generate text sequences or perform cross-language translations effectively. Models can be categorized into unimodal or multimodal. Unimodal models process inputs and generate outputs within the same modality, and multimodal models handle inputs from one modality and produce outputs in a different modality.

## Large Language Models
[MUSIC] Over the past couple of months, large language models, or LLMs, such as ChatGPT, have taken the world by storm. Whether it's writing poetry or helping plan your upcoming vacation, we are seeing a step change in the performance of AI and its potential to drive enterprise value. My name is Kate Soule, I'm a senior manager of business strategy at IBM Research. And today I'm going to give a brief overview of this new field of AI that's emerging and how it can be used in a business setting to drive value. Now, large language models are actually a part of a different class of models called foundation models. Now, the term foundation models was actually first coined by a team from Stanford when they saw that the field of AI was converging to a new paradigm. Where before, AI applications were being built by training, maybe a library of different AI models, where each AI model was trained on very task specific data to perform a very specific task. 

They predicted that we were going to start moving to a new paradigm where we would have a foundational capability, or a foundation model that would drive all of these same use cases and applications. So the same exact applications that we were envisioning before with conventional AI. And the same model could drive any number of additional applications. The point is that this model could be transferred to any number of tasks. That gives this model this superpower to be able to transfer to multiple different tasks and perform multiple different functions is that it's been trained on a huge amount, in an unsupervised manner, on unstructured data. And what that means in the language domain is basically I'll feed a bunch of sentences and I'm talking terabytes of data here to train this model. And the start of my sentence might be no use crying over spilled, and the end of my sentence might be milk. 

And I'm trying to get my model to predict the last word of the sentence based off of the words that it saw before. And it's this generative capability of the model, predicting and generating the next word based off of previous words that it's seen beforehand. That is why that foundation models are actually a part of the field of AI called generative AI, because we're generating something new, in this case the next word in a sentence. And even though these models are trained to perform, at its core, a generation task, predicting the next word in the sentence, we actually can take these models. And if you introduce a small amount of labeled data to the equation, you can tune them to perform traditional NLP tasks. Things like classification or named entity recognition, things that you don't normally associate as being a generative based model or capability. And this process is called tuning, where you can tune your foundation model by introducing a small amount of data. 

You update the parameters of your model, and now it can perform a very specific natural language task. If you don't have data or have only very few data points, you can still take these foundation models, and they actually work very well in low label data domains. And in a process called prompting or prompt engineering, you can apply these models for some of those same exact tasks. So an example of prompting a model to perform a classification task might be, you could give a model a sentence. And then ask it a question, does this sentence have a positive sentiment or negative sentiment? The model is going to try and finish generating words in that sentence. And the next natural word in that sentence would be the answer to your classification problem. 

So it would respond either positive or negative, depending on where it estimated the sentiment of the sentence would be. And these models work surprisingly well when applied to these new settings and domains. Now, this is a lot of where the advantages of foundation models come into play. So if we talk about the advantages, the chief advantage is the performance. These models have seen so much data, again, Data with a capital D, terabytes of data, that by the time that they're applied to small tasks, they can drastically outperform a model that was only trained on just a few data points. The second advantage of these models are the productivity gains. So just like I said earlier, through prompting or tuning, you need far less labeled data to get to a task specific model than if you had to start from scratch. 

Because your model is taking advantage of all the unlabeled data that it saw in its pre-training when we created this generative task. With these advantages, there are also some disadvantages that are important to keep in mind. And the first of those is the compute costs. So the penalty for having this model see so much data is that they're very expensive to train, making it difficult for smaller enterprises to train a foundation model on their own. They're also expensive, by the time they get to a huge size, a couple billion parameters, they're also very expensive to run inference. You might require multiple GPU's at a time just to host these models and run inference, making them a more costly method than traditional approaches. The second disadvantage of these models is on the trustworthiness side. 

So just like data is a huge advantage for these models, they've seen so much unstructured data, it also comes at a cost. Especially in the domain like language, a lot of these models are trained, basically, off of language data that's been scraped from the Internet. And there's so much data that these models have been trained on. Even if you had a whole team of human annotators, you wouldn't be able to go through and actually vet every single data point to make sure that it wasn't biased and contained hate speech or other toxic information. And that's just assuming you actually know what the data is. Often, we don't even know for a lot of these open source models that have been posted, what the exact data sets are that these models have been trained on, leading to trustworthiness issues. So IBM recognizes the huge potential of these technologies. 

But my partners in IBM research are working on multiple different innovations to try and improve also the efficiency of these models and the trustworthiness and reliability of these models to make them more relevant in a business setting. All of these examples that I've talked through so far have just been on the language side. But the reality is there are a lot of other domains that foundation models can be applied towards. Famously, we've seen foundation models for vision, looking at models such as DALL-E 2, which takes text data, and that's then used to generate a custom image. We've seen models for code, with products like Copilot that can help complete code as it's being authored. And IBM's innovating across all of these domains. So whether it's language models that we're building into products like Watson Assistant and Watson Discovery, vision models that we're building into products like Maximo Visual Inspection. 

Or Ansible code models that we're building with our partners at Red Hat under Project Wisdom, we're innovating across all of these domains and more. We're working on chemistry. So, for example, we just published and released MoLFormer, which is a foundation model to promote molecule discovery for different targeted therapeutics. And we're working on models for climate change, building Earth science foundation models using geospatial data to improve climate research. I hope you found this video both informative and helpful. [MUSIC]

## Machine Learning vs. Deep Learning vs. Foundation Models
You've probably seen all AI terms flying around recently, and it can get a little confusing as to how they all relate to one another. Machine learning, deep learning, foundation models, and you've probably seen other terms like generative AI, and large language models. Let's bring an end to the confusion and put these terms in their place. There's one thing they all have in common. They are all terms related to the field of artificial intelligence or AI. Now, AI refers to the simulation of human intelligence in machines, enabling them to perform tasks that typically require human thinking. Now, AI in its various forms and paradigms has been around for decades. 

Perhaps you've heard of the chat bot called Eliza that was developed in the mid 1960s, and that could mimic human like conversation to an extent. Now, a sub field of AI is called machine learning. This sits within the field of AI. Now, what's machine learning? Well, it focuses on developing algorithms that allow computers to learn from and make decisions based upon data. Rather than being explicitly programmed to perform a specific task, these algorithms use statistical techniques to learn patterns in data and make predictions or decisions without human intervention. But like AI, ML or machine learning is a very broad term. 

It encompasses a range of techniques and approaches from traditional statistical methods through to complex neural networks. Now, some of the core categories within ML we can think of are firstly supervised learning. Where models are trained on labeled data. There's also unsupervised learning, and that's where the models find patterns in data without predefined labels, and there's also reinforcement learning. That's where models learn by interacting with an environment and receiving feedback. Where does deep learning come in? Well. Deep learning is a subset of machine learning. Goes right there. 

Now, that specifically focuses on artificial neural networks with multiple layers, and we can think of them looking a bit like this says our nodes and all of our connections. Now, those layers are where we get the deep part from. While traditional ML techniques might be efficient for linear separations or simpler patterns, deep learning excels at handling vast amounts of unstructured data like images or natural language and discovering intricate structures within them. Now, I do want to point out that not all machine learning is deep learning. Traditional machine learning methods still play a pivotal role in many applications. We've got techniques like linear regression. That's a popular technique, or decision trees or support vector machines or clustering algorithms. 

These are all other types of machine learning. They've been widely used for a long time. In some scenarios, look, deep learning might be overkill, or it just isn't the most suitable approach. Machine learning, deep learning. What else? Yeah. Foundation models. 

Where do foundation models fit into this? Well, the term foundation model was popularized in 2021 by researchers at the Stanford Institute, and it fits primarily within the realm of deep learning. I'm going to put foundation models right here. Now, these models are large scale neural networks trained on vast amounts of data, and they serve as a base or a foundation for a multitude of applications. Instead of training a model from scratch for each specific task, you can take a pre trained foundation model and fine tune it for a particular application, which saves a bunch of time and resources. Now, foundation models have been trained on diverse data sets capturing a broad range of knowledge and can be adapted to tasks ranging from language translation to content generation to image recognition. In the grand scheme of things, foundation models, they sit within the deep learning category, but represent a shift towards more generalized, adaptable and scalable AI solutions. 

Look, I think this is hopefully looking a bit clearer now, but there are some other AI related terms. I think it's worth also explaining. One of those is large language models or LLMs. Now, these are a specific type of foundation models, so I've put them in this box here, and they are centered around processing and generating human like text. Let's break it down. LLM, the first L that's large, and that refers to the scale of the model. LLMs possess a vast number of parameters often in the billions or even more. 

This enormity is part of what gives LLMs their nuanced understanding and capability. Second L, that's for language. They're designed to understand and interact using human languages. As they are trained on massive datasets, LLMs can grasp grammar, context, idioms, and even cultural references. The last letter, M, that's for model, at their core, their computational models, a series of algorithms and parameters working together to process input and produce output. LLMs can handle a broad spectrum of language tasks, like answering questions, translating, or even creative writing. Now, if LLMs are one example of foundation models, what are some others? 

Well, there's a bunch. You can think of one of those as being vision models that can see in quotes, interpret and generate images. There are scientific models. Give that an S. Scientific models, for example, are used in biology, where there are models for predicting how proteins fold into three D shapes. There are audio models as well for generating human sounding speech or composing the next fake Drake hit song. Finally, one last term that's gaining traction, we've all heard about it. 

It's generative AI. Now, this term pertains to models and algorithms specifically crafted to generate new content. Essentially, we foundation models provide the underlying structure and understanding, generative AI is about harnessing that knowledge to produce something that is new. It's the creative expression that emerges from the vast knowledge base of these foundation models

## Natural Language Processing, Speech, and Computer Vision
Welcome to the Natural Language Processing, Speech, and Computer Vision. After watching this video, you'll be able to define natural language processing or NLP, speech technology, and computer vision. You'll also be able to analyze their common application areas. Humans have the most advanced method of communication, known as natural language. While humans can use computers or smartphones to send voice and text messages, computers do not innately know how to process natural language. Natural language processing or NLP is a subset of artificial intelligence that enables computers to comprehend, interpret, and produce human language. A global survey on NLP's market size by Fortune Business Insights reveals that the current market size of USD $29.71 billion is likely to go up to USD $158.04 billion in the next eight years, exhibiting a compound annual growth rate or CAGR of 23.2% within the forecast period. 

NLP uses machine learning and deep learning algorithms to discern a word's semantic meaning. It does this by deconstructing sentences grammatically, relationally, and structurally, and understanding the context of use. For instance, based on the context of a conversation, NLP can determine whether the word cloud is a reference to cloud computing or the mass of condensed water vapor floating in the sky. NLP systems also understand intent and emotion, such as whether you're asking a question out of frustration, confusion, or irritation. By understanding the real intent of the user's language, NLP systems draw inferences through a broad array of linguistic models and algorithms. NLP is broken down into many subcategories related to audio and visual tasks. For computers to communicate in natural language, they should be able to convert speech into text, making communication more natural and easier to process. 

They also need to convert text to speech, so users can interact with computers without the requirement to stare at a screen. So let's try to understand speech-to-text and text-to-speech technologies. Speech-to-text or STT technology changes spoken words into written text using neural networks. By analyzing voice samples and their text versions, the neural network identifies patterns in how words are pronounced. It then uses this knowledge to convert new voice recordings into the correct text. STT allows real-time transcription of voice commands, dictation, transcription services, and voice search. For instance, YouTube uses STT to provide automatic closed captioning. 

Virtual assistants like Siri and Google Assistant employ STT to process user commands, and search apps like Google Voice Search use STT to give responses to spoken queries. The flip side of STT is text-to-speech or TTS, also known as speech synthesis. Let's understand how neural networks enable TTS. First, one neural network learns a person's voice by analyzing many voice samples. Then a second neural network generates new audio and checks with the first network to see if it matches the original voice. If it doesn't, the second network adjusts the audio and tries again. This process continues until the generated voice sounds natural and matches the original. 

Together, STT and TTS enable seamless human machine interaction through natural language. Translation services like Google Translate can listen to spoken language through STT, translate it, and then speak the translation through TTS. Smart home devices utilize STT to understand commands and TTS to provide feedback. NLP can also integrate STT and TTS technologies for various applications. In customer support, STT transcribes queries. NLP understands these queries and generates responses, and TTS delivers them. Accessibility, STT transcribes speech into text in real time, which is displayed on a screen or a mobile device. 

NLP analyzes and interprets this, converting it into comprehensible speech via TTS. When you show your face to the phone, have you wondered how your phone magically unlocks? This is made possible by facial recognition, which uses computer vision to analyze your facial image and then match it with a pre-existing one to grant access. So what is computer vision? It is a field of artificial intelligence that empowers machines to interpret and comprehend visual data. It analyzes picture or video data to derive significant conclusions and make judgments. Computer vision is a technology that bridges the digital and physical worlds. 

For instance, it allows self-driving cars to understand and interpret their surroundings. Neural networks are crucial in advancing computer vision applications like image classification, object detection, and image segmentation. Image classification involves dividing images into pre-defined categories, like sorting products on e-commerce platforms or detecting disorders in medical images. Object detection, facilitated by algorithms such as YOLO or you only look once and Faster R-CNN, not only recognizes objects, but also locates them within images, making it essential for areas like surveillance and autonomous vehicles. Image segmentation techniques further analyze visual content by dividing images into meaningful segments. It provides detailed labeling for each pixel, distinguishing between various object types or categories within an image. Let's now see how computer vision is beneficial in various industries. 

In retail, Amazon, Walmart, and Alibaba deploy computer vision for inventory management and personalize shopping experiences, optimizing operations and enhancing customer satisfaction. Manufacturing leaders like Toyota, Siemens, and Bosch integrate computer vision into production lines for quality control and automation, ensuring product consistency and efficiency. Agriculture companies, including John Deere and Monsanto, leverage computer vision for precision farming, enabling farmers to monitor crop health and optimize fields. In this video, you gained insight into how natural language processing aids computers interpreting and producing human language and how it uses machine learning and deep learning algorithms to understand the word's semantic meaning by deconstructing sentences grammatically, relationally, and structurally. Additionally, you learned about STT technology that transforms spoken words into written text, enabling real-time transcription for voice commands, dictation, and voice search. You learned about TTS technology, which converts written text into spoken words, and how it can be integrated with STT for seamless human machine interaction through natural language. Finally, you learned how computer vision enables machines to understand visual data by analyzing images or videos, drawing meaningful insights, and making informed decisions.

## What Is NLP (Natural Language Processing)?
What is natural language processing? Well, you're doing it right now. You're listening to the words and the sentences that I'm forming, and you are forming some comprehension from it. When we ask a computer to do that, that is NLP or natural language processing. My name is Martin Keen, I'm a master inventor at IBM, and I've utilized NLP in a good number of my invention disclosures. NLP really has a really high utility value in all sorts of AI applications. Now, NLP starts with something called unstructured text. 

What is that? Well, that's just what you and I say, that's how we speak. For example, some unstructured text is add eggs and milk to my shopping list. Now, you and I understand exactly what that means, but it is unstructured at least to a computer. What we need to do is to have a structured representation of that same information that a computer can process. Now, that might look something a bit more like this where we have a shopping list element, and then it has subelements within it like an item for eggs and an item for milk. That is an example of something that is structured. 

Now, the job of natural language processing is to translate between these two things. NLP sits right in the middle here, translating between unstructured and structured data. When we go from unstructured here to structured this way, that's called NLU or natural language understanding. When we go this way from structured to unstructured, that's called natural language generation or NLG. We're going to focus today primarily on going from unstructured to structured in natural language processing. Now, let's think of some use cases where NLP might be quite handy. First of all, we've got machine translation. 

Now, when we translate from one language to another, we need to understand the context of that sentence. It's not just a case of taking each individual word from, say, English and then translating it into another language. We need to understand the overall structure and context of what's being said. My favorite example of this going horribly wrong is if you take the phrase, "the spirit is willing, but the flesh is weak" and you translate that from English to Russian, and then you translate that Russian translation back into English, you're going to go from the spirit is willing, but the flesh is weak to something a bit more like the vodka is good, but the meat is rotten, which is really not the intended context of that sentence whatsoever. NLP can help with situations like that. Now, the second use case that I like to mention relates to virtual assistance and also to things like chat bots. Now, a virtual assistant, that's something like Siri or Alexa on your phone that is taking human utterances and deriving a command to execute based upon that. 

A chat bot is something similar except in written language, and that's taking written language and then using it to traverse a decision tree in order to take an action. NLP is very helpful there. Another use case is for sentiment analysis. Now, this is taking some text, perhaps an email message or a product review and trying to derive the sentiment that it's expressed within it. For example, is this product review a positive sentiment or a negative sentiment? Is it written as a serious statement or is it being sarcastic? We can use NLP to tell us. 

Then finally, another good example is spam detection. This is a case of looking at a given email message and trying to derive, is this a real email message or is it spam, and we can look for pointers within the content of the message. Things like over used words or poor grammar or an inappropriate claim of urgency can all indicate that this is actually perhaps spam. Those are some of the things that NLP can provide, but how does it work? Well, the thing with NLP is, it's not like one algorithm. It's actually more like a bag of tools, and you can apply these bag of tools to be able to resolve some of these use cases. Now, the input to NLP is some unstructured text, so either some written text or spoken text that has been converted to a written text through a speech-to-text algorithm. 

Once we've got that, the first stage of NLP is called tokenization. This is about taking a string and breaking it down into chunks. If we consider the unstructured text we've got here, add eggs and milk to my shopping list, that's eight words, that can be eight tokens. From here on in, we are going to work one token at a time as we traverse through this. Now, the first stage once we've got things down into tokens that we can perform is called stemming. This is all about deriving the word stem for a given token. For example, running, runs, and ran, the word stem for all three of those is run. 

We're just removing the prefix and the suffixes and normalizing the tense, and we're getting to the word stem. But stemming doesn't work well for every token. For example, universal and university, well, they don't really stem down to universe. For situations like that, there is another tool that we have available, and that is called lemmatization. Lemmatization takes a given token and learns its meaning through a dictionary definition. From there, it can derive its root or its lemm. Take better, for example. 

Better is derived from good, so the root or the lemm of better is good. The stem of better would be bet. You can see that it is significant whether we use stemming or we use lemmatization for a given token. Now, next thing we can do is we can do a process called part of speech tagging. What this is doing is for a given token is looking where that token is used within the context of a sentence. Take the word make, for example. If I say, "I'm going to make dinner," make is a verb. 

But if I ask you what make is your laptop, well, make is now a noun. Where that token is used in a sentence matters, part of speech tagging can help us derive that context. Then finally, another stage is named entity recognition. What this is asking is for a given token, is there an entity associated with it? For example, a token of Arizona has an entity of a US state, whereas a token of Ralph has an entity of a person's name. These are some of the tools that we can apply in this big bag of tools that we have for NLP in order to get from this unstructured human speech through to something structured that a computer can understand. Once we've done that, then we can apply that structured data to all AI applications. 

But hopefully, this made some sense, and that you were able to process some of the natural language that I shared today. Thanks for watching.

## Self-Driving Cars
Can you tell us a little bit about the work you're doing with self-driving cars? I've been working on self-driving cars for the last few years. It's a domain that's exploded, obviously, in interest since early competitions back in the 2005 domain. What we've been working on really is putting together our own self-driving vehicle that was able to drive on public roads in the region of Waterloo last August. With the self-driving cars area, one of our key research domains is in 3D object detection. This remains a challenging task for algorithms to perform automatically, trying to identify every vehicle, every pedestrian, every sign that's in a driving environment so that the vehicle can make the correct decisions about how it should move and interact with those vehicles. So we work extensively on how we take in laser data, and vision data, and radar data and then fuse that into a complete view of the world around the vehicle. 

When we think of computer vision, we usually think immediately of self-driving cars. Why is that? Well, it's because it's hard to pay attention when driving on the road. You can't both be looking at your smartphone and also be looking at the road at the same time. Of course, it's sometimes hard to predict what people are going to be doing on the street as well when they're crossing the street with their bike or skateboard or whatnot, so it's great when we have some sort of camera or sensor that can help us detect these things and prevent accidents before they could potentially occur. That's one of the limitations of human vision; is visual attention. I could be looking at you, Rav, but behind you could be this delicious slice of pizza. 

But I can only pay attention to one or just some limited number of things at a time. But I can't attend to everything in my visual field all at once, at the same time like a camera could or like how computer vision could potentially do so. That's one of the great things that cameras and computer vision is good for, helping us pay attention to the whole world around us without having us to look around and make sure that we're paying attention to everything. That's just in self-driving cars. I think we all have a good sense of how AI and computer vision shapes the driving and transportation industry. Well, self-driving cars are certainly the future, and there's a tremendous interest right now in self-driving vehicles, in part because of their potential to really change the way our society works and operates. I'm very excited about being able to get into a self-driving car and read or sit on the phone on the way to work, instead of having to pilot through Toronto traffic. 

I think they represent a really exciting step forward, but there's still lots to do. We still have lots of interesting challenges to solve in the self-driving space before we have really robust and safe cars that are able to drive themselves 100% of the time autonomously on our roads.

## AI and Cloud Computing, Edge Computing, and IoT
[MUSIC] Welcome to AI and Cloud Computing, Edge Computing, and IoT. After watching this video, you will be able to describe the basics of IoT, cloud computing, and edge computing, as well as identify their convergence for various use cases. The exciting intersection of AI, cloud computing, edge computing, and IoT brings you smart and real-time applications that change your everyday life. By transforming raw data into meaningful solutions, these technologies work for you to build a smarter and more connected future. Internet of Things, or IoT devices, are all around us in our everyday lives. For instance, fitness trackers and smartwatches that monitor health and activity levels, washing machines that can be operated remotely via smartphone apps. And even smart thermostats that learn your temperature preferences to automatically adjust heating or cooling. 

IoT devices are a network of physical devices connected to the Internet that collect and share data for processing and analysis. These devices can be sensors, cameras, or other devices that generate data. So how do they work? IoT devices collect data and send it via the Internet to the cloud for storage and analysis. Now, cloud computing lets you store and use data and services over the Internet instead of keeping everything on your computer. The main goal of cloud computing is to allow users to access and use powerful computer resources from remote data centers. So it simply means using someone else's computer and software on the Internet instead of your device and resources for computing tasks. 

For example, Gmail is a cloud service that lets you check your email online. Your emails are stored on the cloud so you can access them from anywhere. So what role does AI play in cloud computing? Cloud computing gives us the space to analyze data, while AI algorithms provide smart abilities to find more detailed information in that data, do tasks automatically and make decisions based on data. Together, they help us use data to its fullest. For example, in cloud-based email services, AI analyzes incoming emails to distinguish between spam and legitimate messages. It uses patterns and user behavior to automatically move spam mails to the junk folder. 

This automation improves over time as AI learns from user actions, ensuring users receive fewer unwanted emails. Not all data collected by IoT devices has to go to the cloud. Sometimes edge computing is used, where the device processes and makes decisions locally before sending data to the cloud. This allows faster decision making. So what is edge computing? Edge computing refers to the practice of processing data closer to the source of generation rather than relying on centralized data centers. One of the examples of edge computing can be a simple thermostat. 

Traditionally, a thermostat might rely solely on the cloud to manage your heating and cooling needs. But with edge computing, it can instantly sense the temperature and turn on the heater if it gets too cold. Edge AI is a type of AI that lives on the device itself rather than relying on cloud. Imagine it as a mini brain built into your everyday devices. For instance, security cameras with facial recognition triggering alerts for unauthorized personnel can be one of the examples of edge AI. So how does the intersection of AI, IoT devices, cloud computing, and edge computing work? Let us take an example to understand. 

Imagine you have a fitness tracker that keeps track of your heart rate, steps, and activity level. This fitness tracker is your IoT device that collects and shares your data. With edge computing, your fitness tracker has its own power to do some tasks inside itself. It can count your steps, monitor your heart rate, and analyze this data in real-time without needing to connect to the Internet. For instance, if your heart rate gets too high during exercise, the fitness tracker can alert you right away and suggest slowing down. Now, this fitness tracker can use edge computing along with AI to analyze your activity level, such as recognizing when you are running versus walking, and provide instant coaching instructions. For example, based on the changes in your heart rate patterns, it can ask you to change your workout exercise. 

When you connect your fitness tracker to your smartphone via Bluetooth, you enable cloud computing. Your workout data and health metrics are sent to the cloud, where they can be stored for a long time, and can be analyzed and viewed on your smartphone app. Now, the app can use cloud-based AI to offer advanced features. For instance, it can check how you sleep and give tips to sleep better. Also, it can see your exercise data and make workouts just for you, depending on what you want to achieve. From AI-powered traffic lights and smart public transportation to smart agriculture and AI-based smart buildings, this integration works for you to make your life more efficient. In this video, you learned that IoT devices are a network of physical devices connected to the Internet that collect and share data for processing and analysis. 

Cloud computing allows you to store and use data and services over the Internet. Edge computing refers to the practice of processing data closer to the source of generation rather than relying on centralized data center. The intersection of AI, IoT, cloud computing, and edge computing brings you smart and real-time applications. Real-world applications of AI, cloud computing, edge computing, and IoT can include AI-powered traffic lights, smart public transportation, smart agriculture, and smart buildings. [MUSIC]