---

title: Streams & Buffers in Node.js â€” Concepts, Patterns, Memory, and Pipes  
slug: 32-33_streams-buffers-pipe-memory  
tags:

- topic/nodejs
    
- topic/javascript
    
- topic/streams
    
- topic/performance
    
- level/intermediate  
    created: 2025-10-22  
    updated: 2025-10-22  
    links: []
    

---

# Streams & Buffers in Node.js â€” Concepts, Patterns, Memory, and Pipes

> Goal: Understand **Buffer** vs **Stream**, read/write large files efficiently, control **chunk size** (`highWaterMark`), and **pipe** streams (including on-the-fly compression). Weâ€™ll also compare **memory footprint** of buffered vs streaming approaches.

---

## 1) Concepts at a glance

- **Buffer**: A chunk of raw binary data (Nodeâ€™s `Buffer`); e.g. a snapshot of bytes such as `0x48 0x65 0x6câ€¦`.
    
- **Buffering**: Accumulating data until a certain size before processing/transmitting.
    
- **Stream**: A **sequence of buffers (chunks)** arriving over time. Lets you **process data progressively** (low memory, sooner time-to-first-byte).
    
- **Why streams**: With a 1 GB file, buffering requires ~1 GB RAM; streaming can work with ~KBâ€“MB RAM.
    

---

## 2) Working with `Buffer`

```js
// Create from string / to string
const b = Buffer.from('Convert me to a buffer');
console.log(b.length);                // byte length
console.log(b.toString('utf8'));      // back to string

// Concatenate multiple chunks (e.g., reassemble stream parts)
const chunks = [Buffer.from('Hello, '), Buffer.from('world!')];
const whole = Buffer.concat(chunks);
console.log(whole.toString());

// Allocate an empty buffer (zero-filled vs unsafe)
const safe = Buffer.alloc(1024);          // zero-filled
const fast = Buffer.allocUnsafe(1024);    // faster, but contains old memory (must overwrite!)
```

---

## 3) Reading files: **buffered** vs **streaming**

### 3.1 Buffered (read whole file into memory)

```js
const fs = require('node:fs/promises');

const data = await fs.readFile('big.txt');      // Buffer of entire file
console.log(data.length);
```

- **Pros**: Simple.
    
- **Cons**: Scales poorly; memory = file size.
    

### 3.2 Streaming (progressive, chunked)

```js
const fs = require('node:fs');
const path = require('node:path');

const reader = fs.createReadStream(path.join(__dirname, 'big.txt'), {
  highWaterMark: 16 * 1024, // 16KB chunks (default is ~64KB for files)
});

const chunks = [];
reader.on('data', (chunk) => {
  chunks.push(chunk);             // process/write/pipe instead of storing in real apps
});

reader.on('end', () => {
  console.log('done, chunks:', chunks.length);
});

reader.on('error', (err) => {
  console.error('read error:', err);
});
```

- **Events**: `data` (chunk), `end` (finished), `error` (always handle).
    
- **`highWaterMark`** controls chunk size; _streaming still guarantees order_ of chunks.
    

> Tip: In production, **pipe** to a destination or transform instead of pushing all chunks into an array (which defeats streaming).

---

## 4) Writing files with a **write stream**

```js
const fs = require('node:fs');

const writer = fs.createWriteStream('out.txt');  // accepts Buffers/strings

writer.write('First line\n');
writer.write(Buffer.from('Second line\n'));
writer.end('Final line\n');                      // flush and close

writer.on('finish', () => console.log('write finished'));
writer.on('error',  (e) => console.error('write error:', e));
```

- Events: `finish` (all data flushed), `error`.
    
- Writes can return `false`; respect **backpressure** (wait for `drain`) when writing in tight loops.
    

---

## 5) **Pipe**: connect readable â†’ writable (and transforms)

Simple copy (streaming):

```js
const fs = require('node:fs');

fs.createReadStream('in.txt')
  .pipe(fs.createWriteStream('out.txt'))
  .on('finish', () => console.log('copied'));
```

On-the-fly compression:

```js
const fs = require('node:fs');
const zlib = require('node:zlib');

fs.createReadStream('in.txt', { highWaterMark: 16 * 1024 })
  .pipe(zlib.createGzip())
  .pipe(fs.createWriteStream('in.txt.gz'))
  .on('finish', () => console.log('gzipped'));
```

- You can pipe through multiple transforms (e.g., `read â†’ gunzip â†’ parse â†’ transform â†’ gzip â†’ write`).
    
- Prefer `stream/promises` or `stream.pipeline` for robust piping with proper error propagation:
    

```js
const { pipeline } = require('node:stream/promises');
await pipeline(
  fs.createReadStream('in.txt'),
  zlib.createGzip(),
  fs.createWriteStream('in.txt.gz')
);
```

---

## 6) Memory comparison: buffered vs streaming

```js
const fs = require('node:fs');
const { pipeline } = require('node:stream/promises');

function memMB() {
  return Math.round(process.memoryUsage().rss / 1024 / 1024);
}

// 1) Buffered copy (read entire file, then write)
async function copyBuffered(src, dst) {
  const before = memMB();
  const data = await fs.promises.readFile(src);           // RAM spikes to file size
  await fs.promises.writeFile(dst, data);
  console.log('buffered rss(MB):', before, 'â†’', memMB());
}

// 2) Streaming copy (constant memory)
async function copyStreaming(src, dst) {
  const before = memMB();
  await pipeline(fs.createReadStream(src), fs.createWriteStream(dst));
  console.log('streaming rss(MB):', before, 'â†’', memMB());
}
```

- Expect **buffered** to rise roughly by file size; **streaming** should increase minimally (overheads, not file-sized).
    

---

## 7) Tuning, pitfalls, and best practices

- **Choose chunk size**: Use `highWaterMark` thoughtfully (too small â†’ overhead; too large â†’ memory). Defaults are fine for most cases; increase for throughput-heavy workloads.
    
- **Backpressure**: When `write()` returns `false`, pause reading (or let `pipe` manage it), and resume after `drain`.
    
- **Donâ€™t subvert streaming**: Avoid collecting all chunks into one giant `Buffer` unless you truly need it.
    
- **Always handle errors** on both readable and writable (or use `pipeline`).
    
- **Text vs binary**: For text, you can set `encoding` on streams or call `chunk.toString('utf8')`. For binary, keep `Buffer`s.
    
- **Alternatives**: For pure fileâ†’file copies, `fs.copyFile` is simplest and very fast. Use streams when you need _transformations_ (compression, filtering, parsing) or _progressive I/O_.
    
- **Watching files**: `fs.watch` lets you react to file changes (`change` vs `rename`), but semantics vary by platformâ€”treat as hints and re-check the filesystem.
    

---

## 8) Mini â€œcookbookâ€

**A. Throttle writes with backpressure**

```js
const fs = require('node:fs');

const writer = fs.createWriteStream('big-out.bin');
let i = 0;

function pump() {
  let ok = true;
  while (ok && i < 1_000_000) {
    const buf = Buffer.allocUnsafe(1024); // write 1KB
    ok = writer.write(buf);
    i++;
  }
  if (i < 1_000_000) writer.once('drain', pump);
  else writer.end();
}
pump();
```

**B. Read line-by-line without loading whole file**

```js
const fs = require('node:fs');
const readline = require('node:readline');

const rl = readline.createInterface({
  input: fs.createReadStream('huge.log', { encoding: 'utf8' }),
  crlfDelay: Infinity,
});

let n = 0;
for await (const line of rl) {
  // process line
  n++;
}
console.log('lines:', n);
```

**C. Stream HTTP response (Express example)**

```js
app.get('/download', (req, res) => {
  res.setHeader('Content-Type', 'application/octet-stream');
  fs.createReadStream('big.bin').pipe(res);
});
```

---

## Exercises

1. **Chunk Visualizer**: Create a small file, stream it with `highWaterMark: 16`, and log chunk sizes to see ordering and boundaries.
    
2. **Streaming Transform**: Implement a Transform stream that uppercases ASCII letters and pipe `in.txt â†’ transform â†’ out.txt`.
    
3. **Backpressure Drill**: Write 500 MB in 64 KB chunks to disk; pause on `false`, resume on `drain`. Measure throughput.
    
4. **Compression Bench**: Compare sizes and times of `gzip` vs `brotli` for a large text. (Use `zlib.createBrotliCompress()`.)
    
5. **Memory A/B**: For a ~1 GB test file, measure `rss` change for **buffered copy** vs **streaming copy**.
    

---

## Quick Reference

|Topic|API|Notes|
|---|---|---|
|Buffer basics|`Buffer.from`, `Buffer.concat`, `Buffer.alloc`|Binary data blocks|
|Read stream|`fs.createReadStream(path, { highWaterMark })`|`data`, `end`, `error`|
|Write stream|`fs.createWriteStream(path)`|`write`, `drain`, `finish`, `error`|
|Piping|`readable.pipe(writable)` / `pipeline(...)`|Propagates backpressure|
|Compression|`zlib.createGzip()`, `createBrotliCompress()`|Pipe through for on-the-fly compression|
|Copy (simple)|`fs.copyFile(src, dst)`|Fast fileâ†’file copy|
|Memory usage|`process.memoryUsage().rss`|Rough resident set size in bytes|

---

## ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì„¤ëª… (ì¹œì ˆí•œ ë§ˆë¬´ë¦¬)

ì´ ë…¸íŠ¸ëŠ” **ë²„í¼ì™€ ìŠ¤íŠ¸ë¦¼**ì„ ì§ê´€ì ìœ¼ë¡œ ë¹„êµí•˜ê³ , ì‹¤ì œ ì½”ë“œ íŒ¨í„´ì„ í†µí•´ **ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ì—ì„œì˜ ë©”ëª¨ë¦¬ ì ˆê°** í¬ì¸íŠ¸ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

- **ë²„í¼(Buffer)**ëŠ” í•œ ë²ˆì— ë°ì´í„°ë¥¼ **í†µì§¸ë¡œ** ë©”ëª¨ë¦¬ì— ì˜¬ë¦½ë‹ˆë‹¤. íŒŒì¼ í¬ê¸°ë§Œí¼ì˜ RAMì´ í•„ìš”í•˜ì£ . ë‹¨ìˆœí•˜ì§€ë§Œ 1 GB íŒŒì¼ì´ë©´ RAMë„ 1 GBê°€ í•„ìš”í•´ì„œ ì„œë²„ê°€ ì‰½ê²Œ í„°ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
- **ìŠ¤íŠ¸ë¦¼(Stream)**ì€ ë°ì´í„°ë¥¼ **ì¡°ê°(chunk)**ìœ¼ë¡œ ë‚˜ëˆ  ìˆœì„œëŒ€ë¡œ í˜ë ¤ë³´ëƒ…ë‹ˆë‹¤. `highWaterMark`ë¡œ ì¡°ê° í¬ê¸°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆê³ , **ë©”ëª¨ë¦¬ëŠ” ì¼ì •ëŸ‰ë§Œ ì‚¬ìš©**í•©ë‹ˆë‹¤. ì‹¤ì „ì—ì„œëŠ” íŒŒì´í”„(`pipe`)ë¥¼ ì´ìš©í•´ ì½ê¸°â†’ë³€í™˜â†’ì“°ê¸° íë¦„ì„ êµ¬ì„±í•˜ê³ , ì••ì¶•(`zlib`) ê°™ì€ ì‘ì—…ë„ **ì „ì†¡ ì¤‘ì—** ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
- **í•µì‹¬ ì´ë²¤íŠ¸**: ì½ê¸° ìŠ¤íŠ¸ë¦¼ì€ `data`/`end`/`error`, ì“°ê¸° ìŠ¤íŠ¸ë¦¼ì€ `drain`/`finish`/`error`ë¥¼ ê¼­ ì±™ê¸°ì„¸ìš”. íŠ¹íˆ `write()`ê°€ `false`ë¥¼ ë°˜í™˜í•˜ë©´ **ë°±í”„ë ˆì…”(backpressure)**ê°€ ë°œìƒí•œ ê²ƒì´ë¯€ë¡œ `drain`ì„ ê¸°ë‹¤ë ¸ë‹¤ê°€ ë‹¤ì‹œ ì“°ëŠ” íŒ¨í„´ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    
- **ë©”ëª¨ë¦¬ ë¹„êµ**: í†µë²„í¼ ë°©ì‹ì€ RSSê°€ íŒŒì¼ í¬ê¸°ë§Œí¼ íŠ€ì§€ë§Œ, ìŠ¤íŠ¸ë¦¬ë°ì€ ì˜¤ë²„í—¤ë“œ ì •ë„ë§Œ ì¦ê°€í•©ë‹ˆë‹¤. ëŒ€ìš©ëŸ‰ íŒŒì¼(ìˆ˜ë°± MB~GB) ì²˜ë¦¬ ì‹œ **ìŠ¤íŠ¸ë¦¼ì´ ì‚¬ì‹¤ìƒ í•„ìˆ˜**ì…ë‹ˆë‹¤.
    
- **ì‹¤ë¬´ íŒ**:
    
    - ê²½ëŸ‰ ë³µì‚¬ëŠ” `fs.copyFile`ì´ ë¹ ë¥´ê³  ê°„ë‹¨í•©ë‹ˆë‹¤.
        
    - ë³€í™˜Â·ì••ì¶•Â·í•„í„°ë§ ë“± **ì¤‘ê°„ ì²˜ë¦¬**ê°€ í•„ìš”í•˜ë©´ ìŠ¤íŠ¸ë¦¼ íŒŒì´í”„ë¼ì¸ì„ ì“°ì„¸ìš”.
        
    - ì—ëŸ¬ ì²˜ë¦¬ëŠ” í•­ìƒ ì–‘ìª½ ìŠ¤íŠ¸ë¦¼ì—ì„œ í•˜ê±°ë‚˜ `stream.pipeline`(ë˜ëŠ” `stream/promises`)ë¡œ **ì¢…ë‹¨ ê°„ ì—ëŸ¬ ì „íŒŒ**ë¥¼ ë³´ì¥í•˜ì„¸ìš”.
        
    - í…ìŠ¤íŠ¸ëŠ” ì¸ì½”ë”©(`utf8`) ì„¤ì •ì´ë‚˜ `toString`ìœ¼ë¡œ, ë°”ì´ë„ˆë¦¬ëŠ” `Buffer`ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.
        

ì´ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì€ íŒŒì¼ì€ ë²„í¼ë¡œ ê°„ë‹¨íˆ, í° íŒŒì¼Â·ë„¤íŠ¸ì›Œí¬ ì „ì†¡Â·ì‹¤ì‹œê°„ ì²˜ë¦¬ì—ëŠ” ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ í™•ì¥ì„± ìˆê²Œ êµ¬ì„±í•´ ë³´ì„¸ìš”.