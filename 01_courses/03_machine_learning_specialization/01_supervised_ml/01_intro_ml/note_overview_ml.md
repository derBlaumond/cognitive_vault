## 1.1 머신러닝이란 무엇인가?

머신러닝은 **컴퓨터가 명시적으로 프로그래밍되지 않아도 스스로 학습해 특정 작업을 수행하도록 만드는 기술**이다.  
우리는 일상 속에서 무수히 많은 머신러닝 기반 제품을 무심코 사용하고 있다.

### 대표적인 일상 속 사례

#### 1) 웹 검색

- “How do I make a sushi roll?” 같은 질문을 입력했을 때,  
    Google / Bing / Baidu 등의 검색 엔진은 수많은 웹페이지 중 가장 관련 높은 페이지를 상단에 보여준다.
    
- 이 “순위 결정(rank)” 과정이 바로 머신러닝 알고리즘에 의해 학습된 결과이다.
    

#### 2) 사진 속 얼굴 태깅

- Instagram·Snapchat에 사진을 업로드하면, 앱이 자동으로 친구 얼굴을 인식하고 태그를 추천한다.
    
- 이는 **컴퓨터 비전 기반의 이미지 분류 모델**이 학습 데이터를 통해 얼굴을 인식한 결과다.
    

#### 3) 추천 시스템

- Star Wars를 보고 나면 “비슷한 영화 추천” 기능이 뜨는데,  
    이는 사용자의 시청 이력과 유사 사용자 데이터를 바탕으로 다음 시청 아이템을 예측하는 **추천 알고리즘** 덕분이다.
    

#### 4) 음성 인식

- 휴대폰의 음성 입력(“Hey Siri, play a song by Rihanna!”) 또는  
    “OK Google, Indian restaurants near me” 같은 명령 처리 역시 머신러닝 모델의 출력이다.
    

#### 5) 스팸 필터링

- “Congratulations, you won a million dollars” 같은 이메일이 스팸으로 자동 분류되는 이유는  
    스팸/정상 메일 데이터를 학습한 분류 모델이 작동하기 때문이다.
    

---

## 1.2 산업에서의 머신러닝 활용

머신러닝은 개인 소비자 제품뿐 아니라 산업 현장에서도 빠르게 확산되고 있다.

### 예시

- **기후 변화 대응**: 풍력 터빈의 효율을 자동 최적화하는 시스템
    
- **헬스케어**: 의사의 X-ray 진단 보조 시스템
    
- **제조업 품질 검사**: 생산 라인에서 제품의 스크래치/불량을 자동 탐지하는 컴퓨터 비전 모델
    

Andrew Ng는 Stanford AI Fund · Google Brain · Baidu 등에서 이러한 분야를 직접 연구했고,  
현재도 제조업·대규모 농업·eCommerce 등 다양한 산업과 함께 머신러닝 적용을 진행 중이라고 이야기한다.

---

## 1.3 왜 머신러닝이 중요한가?

### 1) 기존의 프로그래밍 방식으로는 불가능한 문제 해결

- 지도 찾기(GPS 최단 경로)는 명시적 알고리즘으로 구현 가능하지만,  
    **웹 검색, 음성 인식, 자율주행, 의료 영상 분석** 등은  
    규칙을 사람이 직접 작성하는 것이 사실상 불가능하다.
    
- 그나마 해결책은 “데이터로부터 스스로 학습하게 하는 것(머신러닝)”이었다.
    

### 2) AGI로 가는 길에서도 핵심

- 인간 수준의 지능을 가진 AGI(Artificial General Intelligence)는 과장되긴 했지만  
    대부분의 연구자들은 **학습 알고리즘이 AGI로 가는 가장 현실적인 경로**라고 본다.
    

### 3) 경제적 가치

McKinsey 연구에 따르면  
**2030년까지 머신러닝이 창출하는 경제 가치는 연간 약 13조 달러에 이를 것**이라 한다.

### 4) 전 산업적 확장성

- 소프트웨어 산업에서 이미 큰 가치를 만들고 있지만
    
- 제조, 교통, 자동차, 금융, 소매 등 **비(非)소프트웨어 산업에 훨씬 큰 잠재력**이 남아 있다.
    

---

## 1.4 이 강의에서 배우는 것

이 Specialization은 다음 세 가지 축을 중심으로 한다.

### (1) 핵심 알고리즘 배우기

- 실제 Big Tech 기업에서 사용되는 **핵심 ML 알고리즘**을 익힌다.
    

### (2) 직접 구현

- 단순 이론이 아니라 Python으로 직접 모델을 만들어보고 실험한다.
    

### (3) 실전 적용 능력

- 실무에서 실패하지 않도록  
    **데이터 품질 점검, 모델 선택, 평가, 최적화 등 실전 팁**을 상세히 배운다.
    
- 많은 팀이 “잘못된 접근을 6개월 동안 반복”하는 것을 방지하기 위한 내용이 포함됨.
    

---

# Chapter 2. Supervised vs Unsupervised Learning

(파일: 02_supervised_vs_unsupervised.md)

---

## 2.1 머신러닝의 첫 정의 — Arthur Samuel

Arthur Samuel은 머신러닝을 아래와 같이 정의했다:

**“명시적으로 프로그래밍하지 않아도 컴퓨터에 학습 능력을 부여하는 기술”**

그는 스스로도 뛰어난 체크커(player)가 아니었지만  
**체커 게임을 수만 번 스스로 플레이하게 하여 학습하는 프로그램**을 만들었고,  
프로그램이 그의 실력을 넘어서게 되었다.

핵심 포인트:

- 학습 기회가 많을수록 모델의 성능도 좋아진다.
    
- 이 개념이 오늘날 모든 머신러닝의 기반이다.
    

---

## 2.2 머신러닝 알고리즘의 두 축

머신러닝은 크게 **두 가지**로 나뉜다.

### 1) Supervised Learning (지도학습)

- **가장 많은 경제적 가치를 창출**하는 분야 (전체의 99% 이상)
    
- “입력 X → 정답 Y” 쌍이 있는 데이터로부터 학습
    
- 예:
    
    - 스팸 분류
        
    - 음성 → 텍스트
        
    - 이미지 → 라벨
        
    - 광고 클릭 확률 예측
        
    - 집값 예측
        

→ 이 Specialization의 Course 1, 2가 이 내용을 집중적으로 다룸.

### 2) Unsupervised Learning (비지도학습)

- 라벨이 없는 데이터에서 패턴을 찾는 알고리즘
    
- 예:
    
    - 클러스터링
        
    - 이상 탐지
        
    - 차원 축소
        
    - 군집 기반 추천
        

→ Course 3에서 다룸.

---

## 2.3 지도학습의 예 — Regression & Classification

### 1) Regression (회귀)

- 목표: **숫자(Y)를 예측**
    
- 예:
    
    - 집값
        
    - 매출
        
    - 온도
        
    - 거리
        

### 2) Classification (분류)

- 목표: **카테고리(클래스)를 예측**
    
- 예:
    
    - 암 진단 (benign / malignant)
        
    - 이메일 스팸 / 정상
        
    - 이미지: 고양이 vs 개
        
    - 질병 유형 10개 중 하나 선택
        

Regression과 Classification의 가장 큰 차이:

|구분|Regression|Classification|
|---|---|---|
|출력 형태|연속값(무한한 숫자)|유한한 클래스(0,1,2…)|
|예|집값 183,000|암: 악성(1) / 양성(0)|
|특징|숫자 모든 값이 가능|클래스만 존재, 0.5, 1.7 같은 중간 숫자는 의미 없음|

---

## 2.4 다중 입력(feature)이 가능하다

집값 예측 또는 암 진단 모두  
입력이 여러 개일 수 있다.

- 크기(size)
    
- 방 수(bedrooms)
    
- 위치(location)
    
- 환자 나이(age)
    
- 종양 크기(tumor size)
    
- 종양 모양 등(features)
    

입력이 하나면 **univariate**,  
여러 개면 **multivariate** supervised learning이라고 한다.

---

# Chapter 3. Regression Model — Linear Regression

(파일: 03_regression_model.md)

---

## 3.1 Supervised Learning의 전체 흐름

### 1) Training Set 준비

- 입력 x
    
- 출력 y(정답, target)
    

### 2) 모델 선택

- 여기서는 **선형회귀(linear regression)** 선택
    
- y ≈ f(x) = wx + b
    

### 3) 모델 학습

- w, b를 적절히 조정하여  
    **예측 ŷ이 실제 y에 가깝도록 만든다.**
    

### 4) 새 입력에 대해 예측

- 학습된 f(x)를 사용해 새로운 x에 대한 ŷ를 출력한다.
    

---

## 3.2 데이터 예시 — Portland 집값 데이터

- 입력: 집 크기(제곱피트)
    
- 출력: 가격(천 달러 단위)
    
- 총 47개 데이터(m=47)
    

데이터는 **테이블 형태**와 **산점도 그래프** 형태로 모두 확인할 수 있다.

---

## 3.3 머신러닝 표기법 정리 (매우 중요)

|기호|의미|
|---|---|
|x|입력(feature)|
|y|출력(target)|
|m|전체 학습 데이터 개수|
|(xᶦ, yᶦ)|i번째 학습 데이터|
|f(x)|모델(예측 함수)|
|ŷ|예측된 y|

기억해야 할 핵심:

**y = 실제 정답  
ŷ = 모델이 출력한 예측값**

---

## 3.4 Linear Regression 모델

f(x) = wx + b

- w : 기울기
    
- b : y절편
    

이 모델을 사용해 그래프 위에 직선을 하나 긋는 셈이다.  
집 크기(x)가 커질수록 가격(y)도 자연스럽게 증가하므로  
직선이 적절한 모델 형태다.

- 한 개의 입력(feature)만 있으면 **univariate linear regression**
    
- 여러 입력이 있으면 **multivariate linear regression**
    

---

# Chapter 4. Cost Function & Gradient Descent

(파일: 04_train_model_gradient_descent.md)

---

## 4.1 Cost Function (비용 함수)

학습 목표:  
**w, b를 어떻게 조정해야 데이터와 잘 맞는 직선을 만들 수 있을까?**

이를 위해 "오차"를 측정하는 함수가 필요하다.

### 개별 오차(error)

오차 = (예측값 − 실제값)

예:  
ŷᶦ − yᶦ

### 전체 오차를 합치기

모든 데이터의 오차를 제곱하여 더한다.  
(제곱하는 이유: 음수 제거 + 큰 오차에 더 큰 패널티)

### 평균 오차

J(w,b) = (1 / 2m) Σ ( f(xᶦ) − yᶦ )²

이것이 **제곱 오차 비용 함수 (Squared Error Cost Function)**이며  
선형회귀에서 가장 많이 사용된다.

---

## 4.2 비용 함수가 하는 일

비용이 작을수록  
→ 모델의 직선이 데이터에 잘 맞는다.

비용이 크면  
→ 모델이 맞지 않음.

궁극적인 목표는  
**J(w,b)를 최소화하는 w,b를 찾는 것.**

---

## 4.3 Gradient Descent (경사 하강법) 개념

J(w,b)를 최소로 만드는 방법은  
일반적인 해석적 방법으로는 어려움 → **수치적 최적화 필요**

Gradient Descent는 다음과 같은 과정으로 동작한다:

1. w,b 값을 아무 값이나 초기화 (보통 0)
    
2. 비용 함수 J의 기울기(gradient)를 구한다
    
3. 기울기가 내려가는 방향으로 조금씩 이동한다
    
4. 반복
    
5. J가 더 내려가지 않으면 종료(수렴)

Gradient Descent(경사 하강법)는 **함수 J(w,b)의 높이를 최소로 만드는 w,b를 찾는 알고리즘**이다.  
Andrew Ng가 설명한 방식은 다음과 같다:

### 산(또는 골프장 언덕) 비유

- J(w,b)를 3D 표면으로 보면 마치 울퉁불퉁한 언덕/계곡처럼 보인다.
    
- w,b 값을 선택하는 것은 “언덕 특정 지점에 서 있는 것”과 같다.
    
- 목표는 **가능한 가장 낮은 지점(= 최소값)** 에 도달하는 것.
    

Gradient Descent는 다음과 같은 동작을 반복한다:

1. 현재 위치(w,b)에서 360도를 둘러본다.
    
2. “가장 가파르게 내려가는 방향(steepest descent)”을 찾는다.
    
3. 그 방향으로 **작은 한 걸음(learning rate 크기)** 이동한다.
    
4. 새로운 위치에서 다시 반복한다.
    

이 과정은 결국 지역 최소값(local minimum)에 도달하게 된다.

### Multiple Minima (다중 최소값)

지형이 bowl(사발) 모양이면 하나의 최저점만 존재하지만,  
신경망과 같은 복잡한 모델은 다음과 같은 문제가 있다:

- 출발 지점에 따라 **전혀 다른 valley(지역 최소)에 수렴**할 수 있음.
    
- gradient descent는 결국 “지금 있는 곳에서 가장 빠르게 내려가는 방향”만 보기 때문에  
    다른 valley로 이동하지는 않는다.
    

그러나 선형회귀의 비용 함수는 항상 **하나의 전역 최소값**만 있음.  
따라서 초기값에 민감하지 않고 안정적으로 수렴한다.

---

## 4.4 Gradient Descent — 수식 형태의 알고리즘

### 기본 업데이트 식

w := w − α * (∂/∂w) J(w,b)  
b := b − α * (∂/∂b) J(w,b)

- α(alpha): learning rate(학습률) — 한 번에 얼마나 크게 움직일지 결정
    
- (∂/∂w) J(w,b): w 방향의 경사(기울기, gradient)
    
- (∂/∂b) J(w,b): b 방향의 경사(gradient)
    

### Assignment(할당) 의미

Andrew Ng가 강조한 부분:

- 여기서 "=" 는 **수학적 동등(equal)**이 아니라  
    **프로그래밍에서의 대입(assign)** 의미다.
    
- 즉, 오른쪽 값을 계산한 뒤 w에 새로 넣는 것.
    

예시:  
w := w + 1  
→ 기존의 w 값을 읽고, 1 더해 새로운 w에 저장하는 것.

---

## 4.5 Learning Rate α의 의미

### α가 너무 작으면?

- 한 걸음이 너무 작아져 **수렴 속도가 매우 느려진다**
    
- 1%씩만 이동하니 J가 거의 줄지 않음
    

### α가 너무 크면?

- 너무 큰 걸음을 내딛기 때문에
    
- 최소값을 지나쳐 버리고 오히려 J가 증가
    
- 심하면 계속 튀어 다니며 발산(diverge)
    

### α는 어떻게 선택할까?

- 보통 0.1, 0.01, 0.001처럼 **10의 거듭제곱 단위**로 시도
    
- Cost J가 iteration마다 감소하는지 직접 확인하는 것이 핵심
    

---

## 4.6 Derivative(도함수)가 gradient descent에서 가지는 의미

### 도함수 = 기울기(slope)

도함수는 J(w)의 기울기를 나타낸다.

- 기울기 > 0 (양수)  
    → 함수가 오른쪽으로 갈수록 증가  
    → w를 **줄여야(왼쪽으로 이동)** J가 감소  
    → gradient descent 식: w := w − α * (positive)  
    → w 값이 작아지는 방향으로 이동
    
- 기울기 < 0 (음수)  
    → 함수가 오른쪽으로 갈수록 감소  
    → w를 **늘려야(오른쪽으로 이동)** J가 감소  
    → gradient descent 식: w := w − α * (negative)  
    → 음수를 빼면 w 값이 증가
    

결론:  
**도함수는 “지금 어느 방향으로 움직여야 J가 줄어드는지” 알려주는 신호.**

---

## 4.7 1차원 예제로 보는 gradient descent 동작 방식

### 케이스 1: 기울기가 양수일 때

- 현재 지점에서 tangent line의 기울기가 +2 라고 가정
    
- w := w − α * 2
    
- α가 0.1이라면  
    w := w − 0.2  
    → w가 줄어듦 → 왼쪽으로 이동 → J 감소
    

### 케이스 2: 기울기가 음수일 때

- 기울기가 -2라고 가정
    
- w := w − α * (−2) = w + 0.2  
    → w가 증가 → 오른쪽으로 이동 → J 감소
    

이 예시는 **그라디언트가 기울기 방향을 알려주는 역할**이라는 것을 명확히 보여준다.

---

## 4.8 다중 변수의 gradient descent (w1, w2, ..., wn)

선형 회귀가 feature 여러 개를 포함한다면:

- w가 1개가 아니라 w1, w2, ... wn이 된다.
    
- J는 w1 ~ wn, b에 대한 함수 J(w1, …, wn, b)
    
- gradient descent는 모든 w_i에 대해 동시에 업데이트 수행
    

각 파라미터 i에 대해:

w_i := w_i − α * (∂/∂w_i) J  
b := b − α * (∂/∂b) J

즉, **gradient descent는 확장성이 좋다**.  
입력이 1개든 100개든 알고리즘은 동일하게 작동한다.

---

## 4.9 Gradient Descent 구현에서 매우 중요한 개념 — “Simultaneous Update(동시 업데이트)”

Andrew Ng가 매우 강하게 강조한 부분.

### 올바른 구현 방식

1. tempW = w − α * (∂J/∂w)
    
2. tempB = b − α * (∂J/∂b)
    
3. w = tempW
    
4. b = tempB
    

즉,  
**업데이트를 계산할 때는 반드시 이전 단계의 w,b를 기준으로 모두 계산**  
그 후 한 번에 적용한다.

### 잘못된 구현의 문제점

만약

1. tempW 먼저 계산 후 바로 w 갱신
    
2. 그 “변경된 w”를 사용해 tempB 계산
    
3. b 갱신
    

이렇게 하면

- tempB 계산이 begin 단계의 w가 아닌  
    **업데이트된 w**를 사용하기 때문에
    
- 원래 gradient descent와 다른 알고리즘이 되어버린다.
    

결과적으로:

- 실제 gradient descent와 다른 경로를 따른다
    
- 수렴 속도가 이상해지거나 발산 가능성도 있음
    

따라서 반드시 **동시 업데이트(simultaneous update)**가 필요하다.

---

# Chapter 4 마무리 — 핵심 요약

### 1) Cost Function

J(w,b) = (1/2m) Σ (f(xᶦ) − yᶦ)²  
→ 모델이 얼마나 잘못 예측하는지 측정하는 기준

### 2) Goal

J(w,b)를 최소화하는 w,b 찾기

### 3) Gradient Descent 핵심 업데이트

w := w − α * (∂J/∂w)  
b := b − α * (∂J/∂b)

### 4) α (learning rate)의 역할

- 너무 크면 발산
    
- 너무 작으면 매우 느림
    
- 0.1, 0.01, 0.001 등 시도하며 조정
    

### 5) 직관

“지금 있는 장소에서 가장 가파르게 내려가는 방향으로 한 걸음”  
→ 이 과정을 반복하면 J가 줄어든다.

### 6) Simultaneous Update 필수

모든 파라미터는 **이전 값(w_old, b_old)** 기반으로  
gradient 계산 후  
한 번에 업데이트해야 한다.
